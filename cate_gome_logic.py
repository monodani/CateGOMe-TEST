# /cate_gome_logic.py (FINAL VERSION - ALL-IN-ONE)

import streamlit as st
import os
import re
import json
import requests
import pandas as pd
from typing import List, Dict, Any
from pathlib import Path

# LangChain ë° AI ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
import google.generativeai as genai

# --- ì„¤ì •ê°’ (Configuration) ---
EMBED_MODEL = "text-embedding-3-large"
LLM_MODEL = "gpt-4o"
GEMINI_MODEL = "gemini-1.5-flash"
NUM_RELATED_TERMS = 3
SIMILARITY_TOP_K = 3


@st.cache_resource
def load_all_data_and_models(openai_api_key: str):
    """
    ì•±ì— í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ëŠ” ë‹¨ì¼ í•¨ìˆ˜.
    @st.cache_resource ë•ë¶„ì— ì•± ì„¸ì…˜ ë™ì•ˆ ë”± í•œ ë²ˆë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    # 1. ê²½ë¡œ ë° íŒŒì¼ URL ì •ì˜
    BASE_DIR = Path.cwd() 
    DATA_FILES = {
        # Core Data
        "cases_faiss": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/vectorstores/cases/cases_index.faiss", "path": BASE_DIR / "vectorstores/cases/cases_index.faiss"},
        "cases_pkl": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/vectorstores/cases/cases_index.pkl", "path": BASE_DIR / "vectorstores/cases/cases_index.pkl"},
        "classification_faiss": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/vectorstores/classification/classification_index.faiss", "path": BASE_DIR / "vectorstores/classification/classification_index.faiss"},
        "classification_pkl": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/vectorstores/classification/classification_index.pkl", "path": BASE_DIR / "vectorstores/classification/classification_index.pkl"},
        "classification_csv": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/data/classification_code.csv", "path": BASE_DIR / "data/classification_code.csv"},
        
        # <<< ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì—ˆë˜ ì´ë¯¸ì§€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ëª©ë¡ì…ë‹ˆë‹¤.
        "logo_main": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/assets/CateGOMe/CateGOMe_kor.png", "path": BASE_DIR / "assets/CateGOMe_kor.png"},
        "emoji_hi": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/assets/emoji/CateGOMe_emoji_hi.png", "path": BASE_DIR / "assets/emoji/CateGOMe_emoji_hi.png"},
        "emoji_categorizing": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/assets/emoji/CateGOMe_emoji_categorying.png", "path": BASE_DIR / "assets/emoji/CateGOMe_emoji_categorying.png"},
        "emoji_sorry": {"url": "https://raw.githubusercontent.com/monodani/CateGOMe-TEST/main/assets/emoji/CateGOMe_emoji_sorry.png", "path": BASE_DIR / "assets/emoji/CateGOMe_emoji_sorry.png"}
    }

    # 2. íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë¡œì§
    for key, info in DATA_FILES.items():
        local_path = info["path"]
        if not local_path.exists():
            local_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                with requests.get(info["url"], stream=True) as r:
                    r.raise_for_status()
                    with open(local_path, "wb") as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
            except Exception as e:
                # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì•±ì„ ì¤‘ì§€ì‹œí‚¤ëŠ” ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ
                st.error(f"í•„ìˆ˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {local_path}. ì•±ì„ ì¬ë¶€íŒ…í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {e}")
                st.stop()

    # 3. ë°ì´í„° ë° ëª¨ë¸ ë¡œë”©
    try:
        embeddings = OpenAIEmbeddings(model=EMBED_MODEL, openai_api_key=openai_api_key)

        vs_cases = FAISS.load_local(
            str(BASE_DIR / "vectorstores/cases"),
            embeddings,
            index_name="cases_index",
            allow_dangerous_deserialization=True,
        )
        vs_classification = FAISS.load_local(
            str(BASE_DIR / "vectorstores/classification"),
            embeddings,
            index_name="classification_index",
            allow_dangerous_deserialization=True,
        )
        vectorstores = {"cases": vs_cases, "classification": vs_classification}

        df = pd.read_csv(DATA_FILES["classification_csv"]["path"], encoding="utf-8")
        df = df.reset_index(drop=False).rename(columns={"index": "_rowid"})

        df["ì…ë ¥ì½”ë“œ_str"] = df["ì…ë ¥ì½”ë“œ"].astype(str).str.replace(r"\.0$", "", regex=True)
        code_to_name_map = pd.Series(df.í•­ëª©ëª….values, index=df.ì…ë ¥ì½”ë“œ_str).to_dict()

        # ìµœì‹  langchain_openai ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜ (model=)
        llm_model = ChatOpenAI(model=LLM_MODEL, temperature=0.1, openai_api_key=openai_api_key)

        # ë¡œë“œëœ ëª¨ë“  ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
        return {
            "embeddings": embeddings,
            "vectorstores": vectorstores,
            "df": df,
            "llm_model": llm_model,
            "code_to_name_map": code_to_name_map,
        }
    except Exception as e:
        st.error(f"ë°ì´í„° ë˜ëŠ” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. ì•±ì„ ì¬ë¶€íŒ…í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {e}")
        st.stop()


# --- í—¬í¼ í•¨ìˆ˜ë“¤ ---
def _short_doc_from_row(row: pd.Series) -> Document:
    source = row.get("ì¶œì²˜", "í•­ëª©ë¶„ë¥˜ì§‘")
    source_info = f"ì¶œì²˜: {source}\n"
    core_fields_order = [
        col
        for col in ["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "í•­ëª©ë¶„ë¥˜ë‚´ìš©", "ì²˜ë¦¬ì½”ë“œ", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"]
        if col in row.index
    ]
    core_lines = [f"{col}: {str(row[col])}" for col in core_fields_order]
    page = source_info + "\n".join(core_lines)
    meta = row.to_dict()
    return Document(page_content=page, metadata=meta)


def _keyword_search(df: pd.DataFrame, term: str) -> List[Document]:
    if df is None:
        return []
    df_copy = df.copy()
    search_cols = ["í•­ëª©ë¶„ë¥˜ë‚´ìš©", "í•­ëª©ëª…", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"]
    for c in search_cols:
        if c in df_copy.columns:
            df_copy[c] = df_copy[c].astype(str)

    # regex=False ì¶”ê°€í•˜ì—¬ UserWarning ë°©ì§€
    mask = (
        df_copy["í•­ëª©ë¶„ë¥˜ë‚´ìš©"].str.contains(term, case=False, na=False, regex=False)
        | df_copy["í•­ëª©ëª…"].str.contains(term, case=False, na=False, regex=False)
        | df_copy["í¬í•¨í•­ëª©"].str.contains(term, case=False, na=False, regex=False)
        | df_copy["ì œì™¸í•­ëª©"].str.contains(term, case=False, na=False, regex=False)
    )
    sub = df_copy.loc[mask].drop_duplicates(subset=["_rowid"], keep="first")
    return [_short_doc_from_row(r) for _, r in sub.iterrows()]


def _keyword_search_on_docs(docs: List[Document], term: str) -> List[Document]:
    if not docs:
        return []
    return [doc for doc in docs if term.lower() in doc.page_content.lower()]


def _similarity_topk_for_term(vs: FAISS, embeddings: OpenAIEmbeddings, term: str, k: int) -> List[Document]:
    if vs is None or embeddings is None:
        return []
    retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": k, "fetch_k": 30, "lambda_mult": 0.5})
    return retriever.invoke(term)


def _get_term_info_via_llm(llm: ChatOpenAI, user_query: str, num_related_terms: int) -> List[Dict[str, Any]]:
    # ... (ë‚´ìš© ë³€ê²½ ì—†ìŒ)
    prompt = f"""ë„ˆëŠ” ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ 'product_name' ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  í’ˆëª©ëª…ì„ ë¶„ì„í•˜ê³ , ì˜¤íƒˆìë¥¼ êµì •í•œ ë’¤ ê²€ìƒ‰ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ AIì´ë‹¤. ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì•„ë˜ "ì¶œë ¥ ì˜ˆì‹œ"ì™€ ì™„ë²½í•˜ê²Œ ë™ì¼í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•œë‹¤.
## ì…ë ¥ ì¿¼ë¦¬ ì˜ˆì‹œ:
'''product_name = ['ìŠ¤íƒ€ë²…ìŠ¤ì¡°ê°ì¼€ìµ', 'ì „ê¸°ì°¨ ì¶©ì „', 'ìƒ‰ì§€í”¼í‹°'] ...'''
### ë„ˆì˜ JSON ì¶œë ¥ ì˜ˆì‹œ:
```json
{{
  "terms": [
    {{
      "original_term": "ìŠ¤íƒ€ë²…ìŠ¤ì¡°ê°ì¼€ìµ", "term": "ìŠ¤íƒ€ë²…ìŠ¤ ì¡°ê°ì¼€ìµ", "description": "ìŠ¤íƒ€ë²…ìŠ¤ ì»¤í”¼ ì „ë¬¸ì ì—ì„œ íŒë§¤í•˜ëŠ” ì¡°ê° í˜•íƒœì˜ ì¼€ì´í¬ì…ë‹ˆë‹¤. ì»¤í”¼ì™€ í•¨ê»˜ ì¦ê¸°ëŠ” ëŒ€í‘œì ì¸ ë””ì €íŠ¸ ë©”ë‰´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.", "related_terms": ["ìŠ¤íƒ€ë²…ìŠ¤", "ì¼€ì´í¬", "ë””ì €íŠ¸"]
    }},
    {{
      "original_term": "ì „ê¸°ì°¨ ì¶©ì „", "term": "ì „ê¸°ì°¨ ì¶©ì „", "description": "ì „ê¸° ìë™ì°¨ì˜ ë°°í„°ë¦¬ì— ì „ë ¥ì„ ê³µê¸‰í•˜ëŠ” í–‰ìœ„ì…ë‹ˆë‹¤. ìë™ì°¨ë¥¼ ìš´í–‰í•˜ê¸° ìœ„í•œ ì—ë„ˆì§€ë¥¼ ì±„ìš°ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.", "related_terms": ["ì „ê¸°ì°¨", "ì¶©ì „ì†Œ", "ì—°ë£Œ"]
    }},
    {{
      "original_term": "ìƒ‰ì§€í”¼í‹°", "term": "ì±—ì§€í”¼í‹°", "description": "OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.", "related_terms": ["ìƒì„±í˜•AI", "AI", "LLM"]
    }}
  ]
}}
ì´ì œ ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•´ë¼. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ê³  JSONë§Œ ì¶œë ¥í•´ë¼. ê´€ë ¨ ìš©ì–´ëŠ” {num_related_terms}ê°œ ì¶”ì¶œí•´ë¼.
ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬: {user_query}"""

    try:
        res = llm.invoke(prompt)
        text_content = res.content.strip()
        json_match = re.search(r"\{.*\}", text_content, re.DOTALL)
        if not json_match:
            raise json.JSONDecodeError("No JSON found", text_content, 0)
        data = json.loads(json_match.group(0))
        return data.get("terms", [])
    except Exception as e:
        print(f"ERROR[llm]: Failed to get term info: {e}")
        match = re.search(r"product_name\s*=\s*\[\s*[\"']([^\"']+)[\"']\s*\]", user_query)
        term = match.group(1) if match else "unknown"
        return [{"term": term, "description": "", "related_terms": []}]


# --- ë©”ì¸ ë¡œì§ í•¨ìˆ˜ ---
def get_classification_report(image_bytes: bytes, genai_api_key: str, loaded_data: dict) -> str:
    # ë¡œë“œëœ ê°ì²´ë“¤ì„ ì¸ìë¡œ ë°›ìŒ
    _llm_model = loaded_data["llm_model"]
    _df = loaded_data["df"]
    _vectorstores = loaded_data["vectorstores"]
    _embeddings = loaded_data["embeddings"]
    _code_to_name_map = loaded_data["code_to_name_map"]

    # 1. ì´ë¯¸ì§€ ë¶„ì„
    # ... (ë‚´ìš© ë³€ê²½ ì—†ìŒ)
    try:
        genai.configure(api_key=genai_api_key)
        gemini_model = genai.GenerativeModel(GEMINI_MODEL)
        prompt = """ê°€ê³„ë¶€ ì‚¬ì§„ì—ì„œ í‘œë¥¼ ì¸ì‹í•´ì„œ ê° í–‰ì˜ 1) í’ˆëª©ëª…(= 'ìˆ˜ì…ì¢…ë¥˜ ë° ì§€ì¶œì˜ í’ˆëª…ê³¼ ìš©ë„' ì—´), 2) ìˆ˜ì… ê¸ˆì•¡, 3) ì§€ì¶œ ê¸ˆì•¡ì„ ì¶”ì¶œí•˜ë¼.

ê·œì¹™: ê¸ˆì•¡ì˜ ì‰¼í‘œ(,)ëŠ” ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ. ê°’ì´ ë¹„ì–´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ. ì œëª©í–‰Â·ì²´í¬ë°•ìŠ¤Â·ë¹ˆì¤„ì€ ì œì™¸. ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥.
JSON ìŠ¤í‚¤ë§ˆ: {"items": [{"name": "í’ˆëª©ëª…", "income": 0, "expense": 0}, ...]}"""

        resp = gemini_model.generate_content(
            [prompt, {"mime_type": "image/jpeg", "data": image_bytes}],
            generation_config={"response_mime_type": "application/json"},
        )
        data = json.loads(resp.text)
        items = data.get("items", [])
        if not items:
            return "### ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼\n\nì´ë¯¸ì§€ì—ì„œ ê°€ê³„ë¶€ ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        product_name_list, income_list, expense_list = [], [], []
        for item in items:
            name = str(item.get("name", "")).strip()
            if name:
                product_name_list.append(name)
                income_list.append(int(str(item.get("income", 0)).replace(",", "")))
                expense_list.append(int(str(item.get("expense", 0)).replace(",", "")))
    except Exception as e:
        return f"### ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜\n\nì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # 2. ê°œë³„ í’ˆëª© ì²˜ë¦¬
    prompt_template_single = PromptTemplate.from_template(
        """SYSTEM: ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ 'ì…ë ¥ì½”ë“œ'ì™€ 'í•­ëª©ëª…'ì„ ì¶”ë¡ í•˜ëŠ”, ê·¹ë„ë¡œ ê¼¼ê¼¼í•˜ê³  ê·œì¹™ì„ ì—„ìˆ˜í•˜ëŠ” ë°ì´í„° ë¶„ë¥˜ AIì´ë©°, ë‹¹ì‹ ì˜ ì´ë¦„ì€ "ì¹´í…Œê³ ë¯¸(CateGOMe)"ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

[ì ˆëŒ€ ê·œì¹™ 1] expense > 0 ì´ë©´ input_code >= 1000, income > 0 ì´ë©´ input_code < 1000 ì…ë‹ˆë‹¤.
[ì ˆëŒ€ ê·œì¹™ 2] 'ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘' ì •ë³´ëŠ” 'ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘' ì •ë³´ë³´ë‹¤ í•­ìƒ ìš°ì„ í•©ë‹ˆë‹¤.
[ë¶„ë¥˜ íƒ€ì…] 90% ì´ìƒ í™•ì‹ í•  ìˆ˜ ìˆìœ¼ë©´ 'DEFINITE', í›„ë³´ê°€ ì—¬ëŸ¬ ê°œì´ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ 'AMBIGUOUS'ë¡œ ê²°ì •í•˜ì„¸ìš”. (ì˜ˆ: 'ë„¤ì´ë²„' -> ì„œë¹„ìŠ¤ê°€ ë‹¤ì–‘í•´ ëª¨í˜¸í•¨)
[ì¶œë ¥ í˜•ì‹] ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ 'DEFINITE' ë˜ëŠ” 'AMBIGUOUS' í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
HUMAN: #Question: {question}\n#Context: {context}\nAnswer:"""
    )

    classification_chain_single = (
        {"question": itemgetter("question"), "context": itemgetter("context")}
        | prompt_template_single
        | _llm_model
        | StrOutputParser()
    )

    definite_results, ambiguous_results, failed_results = [], [], []
    all_docs_from_vs = {name: list(vs.docstore._dict.values()) for name, vs in _vectorstores.items()}

    for i, product_name_original in enumerate(product_name_list):
        q_single = f"product_name = ['{product_name_original}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"

        # ê²€ìƒ‰ ë¡œì§
        extracted_terms_info = _get_term_info_via_llm(
            _llm_model, q_single, num_related_terms=NUM_RELATED_TERMS
        )
        search_context_docs = []
        seen_docs_page_content = set()
        for item in extracted_terms_info:
            # ... (ì´í•˜ ê²€ìƒ‰ ë¡œì§ì€ search_classification_codes í•¨ìˆ˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´. ë³€ê²½ ì—†ìŒ)
            term = item.get("term", "")
            if not term:
                continue
            terms_to_search = [term] + item.get("related_terms", [])
            for search_term in terms_to_search:
                for doc in _keyword_search(_df, search_term):
                    if doc.page_content not in seen_docs_page_content:
                        search_context_docs.append(doc)
                        seen_docs_page_content.add(doc.page_content)
            for vs_name, doc_list in all_docs_from_vs.items():
                for search_term in terms_to_search:
                    for doc in _keyword_search_on_docs(doc_list, search_term):
                        new_doc = Document(
                            page_content=f"ì¶œì²˜: {vs_name}\n{doc.page_content}",
                            metadata=doc.metadata,
                        )
                        if new_doc.page_content not in seen_docs_page_content:
                            search_context_docs.append(new_doc)
                            seen_docs_page_content.add(new_doc.page_content)
            description = item.get("description", "")
            if description:
                for vs_name, vs in _vectorstores.items():
                    for doc in _similarity_topk_for_term(
                        vs, _embeddings, description, k=SIMILARITY_TOP_K
                    ):
                        new_doc = Document(
                            page_content=f"ì¶œì²˜: {vs_name}\n{doc.page_content}",
                            metadata=doc.metadata,
                        )
                        if new_doc.page_content not in seen_docs_page_content:
                            search_context_docs.append(new_doc)
                            seen_docs_page_content.add(new_doc.page_content)

        try:
            product_name_corrected = extracted_terms_info[0]["term"]
        except (IndexError, KeyError):
            product_name_corrected = product_name_original

        if not search_context_docs:
            failed_results.append(
                {
                    "í’ˆëª©ëª…": product_name_corrected,
                    "ìˆ˜ì…": income_list[i],
                    "ì§€ì¶œ": expense_list[i],
                    "ì‹¤íŒ¨ ì´ìœ ": "ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨",
                }
            )
            continue

        context = (
            "\n\n---\n\n".join([doc.page_content for doc in search_context_docs])
            .replace("ì¶œì²˜: cases", "ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘")
            .replace("ì¶œì²˜: classification", "ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘")
        )
        final_question = (
            f"product_name = ['{product_name_corrected}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
        )

        try:
            # ... (ì´í•˜ ë¶„ë¥˜ ë° ê²°ê³¼ ìˆ˜ì§‘ ë¡œì§ ë³€ê²½ ì—†ìŒ)
            output_json_str = classification_chain_single.invoke(
                {"question": final_question, "context": context}
            )
            json_match = re.search(r"\{.*\}", output_json_str, re.DOTALL)
            if not json_match:
                raise ValueError("LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            llm_result = json.loads(json_match.group(0))

            if llm_result.get("classification_type") == "DEFINITE":
                res = llm_result.get("result", {})
                code = str(res.get("input_code", "N/A")).strip()
                definite_results.append(
                    {
                        "í’ˆëª©ëª…": product_name_corrected,
                        "ì…ë ¥ì½”ë“œ": code,
                        "í•­ëª©ëª…": _code_to_name_map.get(code, "í•­ëª©ëª… ì—†ìŒ"),
                        "ìˆ˜ì…": income_list[i],
                        "ì§€ì¶œ": expense_list[i],
                        "ì‹ ë¢°ë„": res.get("confidence", "N/A"),
                        "ì¶”ë¡  ì´ìœ ": res.get("reason", "N/A"),
                        "ê·¼ê±° ì •ë³´": res.get("evidence", "N/A"),
                    }
                )
            elif llm_result.get("classification_type") == "AMBIGUOUS":
                candidates = llm_result.get("candidates", [])
                for cand in candidates:
                    cand_code = str(cand.get("input_code", "")).strip()
                    cand["í•­ëª©ëª…"] = _code_to_name_map.get(cand_code, "í•­ëª©ëª… ì—†ìŒ")
                ambiguous_results.append(
                    {
                        "í’ˆëª©ëª…": product_name_corrected,
                        "ìˆ˜ì…": income_list[i],
                        "ì§€ì¶œ": expense_list[i],
                        "ëª¨í˜¸ì„± ì´ìœ ": llm_result.get("reason_for_ambiguity", "N/A"),
                        "í›„ë³´": candidates,
                        "ê·¼ê±° ì •ë³´": llm_result.get("evidence", "N/A"),
                    }
                )
            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” classification_type: {llm_result.get('classification_type')}")
        except Exception as e:
            failed_results.append(
                {
                    "í’ˆëª©ëª…": product_name_corrected,
                    "ìˆ˜ì…": income_list[i],
                    "ì§€ì¶œ": expense_list[i],
                    "ì‹¤íŒ¨ ì´ìœ ": f"LLM ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}",
                }
            )

    # 3. ë³´ê³ ì„œ ìƒì„±
    # ... (ë‚´ìš© ë³€ê²½ ì—†ìŒ)
    report = ["## ğŸ“Š ì¹´í…Œê³ ë¯¸ ë¶„ë¥˜ ê²°ê³¼ ë³´ê³ ì„œ"]
    if definite_results:
        report.append("\n### 1. ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©\n")
        df_definite = pd.DataFrame(definite_results)
        report.append("#### í’ˆëª©ë³„ ë¶„ë¥˜ ê²°ê³¼")
        report.append(df_definite[["í’ˆëª©ëª…", "ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ìˆ˜ì…", "ì§€ì¶œ"]].to_markdown(index=False))
        df_summary = df_definite[pd.to_numeric(df_definite["ì…ë ¥ì½”ë“œ"], errors="coerce").notna()].copy()
        if not df_summary.empty:
            df_summary["ì…ë ¥ì½”ë“œ"] = df_summary["ì…ë ¥ì½”ë“œ"].astype(int)
            df_summary_agg = (
                df_summary.groupby("ì…ë ¥ì½”ë“œ")
                .agg(
                    í•­ëª©ëª…=("í•­ëª©ëª…", "first"),
                    ìˆ˜ì…í•©ê³„=("ìˆ˜ì…", "sum"),
                    ì§€ì¶œí•©ê³„=("ì§€ì¶œ", "sum"),
                    í•´ë‹¹í’ˆëª©ëª…=("í’ˆëª©ëª…", lambda x: ", ".join(x)),
                )
                .reset_index()
            )
            report.append("\n#### ì…ë ¥ì½”ë“œë³„ ìš”ì•½ ê²°ê³¼")
            report.append(df_summary_agg.to_markdown(index=False))
    else:
        report.append("\nëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©ì´ ì—†ìŠµë‹ˆë‹¤.\n")

    if ambiguous_results:
        report.append("\n\n### 2. ì‚¬ìš©ìì˜ ê²€í† ê°€ í•„ìš”í•œ í’ˆëª©\n")
        report.append("> ì•„ë˜ í’ˆëª©ë“¤ì€ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹¨ì¼ ì½”ë“œë¥¼ í™•ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n")
        for res in ambiguous_results:
            report.append(
                f"---\n**í’ˆëª©ëª…: {res['í’ˆëª©ëª…']}** (ìˆ˜ì…: {res['ìˆ˜ì…']:,}ì›, ì§€ì¶œ: {res['ì§€ì¶œ']:,}ì›)"
            )
            report.append(f"**- ê²€í†  í•„ìš” ì´ìœ :** {res['ëª¨í˜¸ì„± ì´ìœ ']}")
            if res.get("í›„ë³´"):
                df_cand = pd.DataFrame(res["í›„ë³´"]).rename(
                    columns={"input_code": "ì…ë ¥ì½”ë“œ", "confidence": "ì‹ ë¢°ë„", "reason": "ì´ìœ "}
                )
                report.append(df_cand[["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ì´ìœ "]].to_markdown(index=False))

    if definite_results:
        report.append("\n\n### 3. ëª…í™•í•œ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±°\n")
        for res in definite_results:
            report.append(f"---\n**í’ˆëª©ëª…: {res['í’ˆëª©ëª…']} (ì„ íƒëœ ì½”ë“œ: {res['ì…ë ¥ì½”ë“œ']})**")
            report.append(f"**- ì¶”ë¡  ì´ìœ :** {res['ì¶”ë¡  ì´ìœ ']}")
            report.append(f"**- í•µì‹¬ ê·¼ê±°:**\n```\n{res['ê·¼ê±° ì •ë³´']}\n```")

    if failed_results:
        report.append("\n\n### 4. ì²˜ë¦¬ ì‹¤íŒ¨ í•­ëª©\n")
        report.append(pd.DataFrame(failed_results).to_markdown(index=False))

    return "\n".join(report)

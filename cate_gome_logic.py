# /cate_gome_logic.py

import os
import re
import json
import ast
import pandas as pd
from typing import List, Dict, Any

# LangChain ë° AI ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
import google.generativeai as genai

# --- ì„¤ì •ê°’ (Configuration) ---
# ì´ ë¶€ë¶„ì˜ ê°’ì„ ë³€ê²½í•˜ì—¬ ëª¨ë¸ì˜ ë™ì‘ì„ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# 1. AI ëª¨ë¸ ì„¤ì •
# OpenAI ì„ë² ë”© ëª¨ë¸: ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
EMBED_MODEL = "text-embedding-3-large"
# OpenAI LLM ëª¨ë¸: ìµœì¢… ë‹µë³€ ìƒì„± ë° í‚¤ì›Œë“œ ì¶”ì¶œì— ì‚¬ìš©ë©ë‹ˆë‹¤.
LLM_MODEL = "gpt-4o"
# Google Gemini ëª¨ë¸: ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸(ê°€ê³„ë¶€ ë‚´ì—­)ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
GEMINI_MODEL = "gemini-1.5-flash"

# 2. ë²¡í„°ìŠ¤í† ì–´ ë° ë°ì´í„° ê²½ë¡œ ì„¤ì • (utils.pyì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ê²½ë¡œ)
VECTORSTORE_DIR_CASES = "vectorstores/cases"
INDEX_NAME_CASES = "case_index"
VECTORSTORE_DIR_CLASSIFICATION = "vectorstores/classification"
INDEX_NAME_CLASSIFICATION = "classification_index"
CSV_PATH = "data/classification_code.csv"

# 3. ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„°
# ê° í’ˆëª©ì— ëŒ€í•´ LLMì´ ìƒì„±/ì¶”ì¶œí•  ê´€ë ¨ ìš©ì–´ì˜ ìµœëŒ€ ê°œìˆ˜
NUM_RELATED_TERMS = 3
# ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œ ê°€ì ¸ì˜¬ ìƒìœ„ ê²°ê³¼ì˜ ê°œìˆ˜
SIMILARITY_TOP_K = 3

# --- ì „ì—­ ë³€ìˆ˜ ë° ì´ˆê¸°í™” ---
# ì´ ë³€ìˆ˜ë“¤ì€ ì•± ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ë¡œë“œë˜ì–´ ë©”ëª¨ë¦¬ì— ìºì‹œë©ë‹ˆë‹¤.
_embeddings = None
_vectorstores = None
_df = None
_llm_model = None
_code_to_name_map = None

def initialize_models_and_data(openai_api_key: str):
    """
    AI ëª¨ë¸, ë²¡í„°ìŠ¤í† ì–´, ë°ì´í„°í”„ë ˆì„ ë“± í•µì‹¬ êµ¬ì„±ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ë©”ì¸ ì•±ì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global _embeddings, _vectorstores, _df, _llm_model, _code_to_name_map
    
    try:
        # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        _embeddings = OpenAIEmbeddings(model=EMBED_MODEL, openai_api_key=openai_api_key)

        # ë‘ ê°œì˜ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        vs_cases = FAISS.load_local(
            folder_path=VECTORSTORE_DIR_CASES,
            embeddings=_embeddings,
            index_name=INDEX_NAME_CASES,
            allow_dangerous_deserialization=True
        )
        vs_classification = FAISS.load_local(
            folder_path=VECTORSTORE_DIR_CLASSIFICATION,
            embeddings=_embeddings,
            index_name=INDEX_NAME_CLASSIFICATION,
            allow_dangerous_deserialization=True
        )
        _vectorstores = {"cases": vs_cases, "classification": vs_classification}

        # CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        _df = pd.read_csv(CSV_PATH, encoding='utf-8')
        _df = _df.reset_index(drop=False).rename(columns={"index": "_rowid"})
        
        # ì…ë ¥ì½”ë“œ-í•­ëª©ëª… ë§µ ìƒì„± (ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•¨)
        _df['ì…ë ¥ì½”ë“œ_str'] = _df['ì…ë ¥ì½”ë“œ'].astype(str).str.replace(r'\.0$', '', regex=True)
        _code_to_name_map = pd.Series(_df.í•­ëª©ëª….values, index=_df.ì…ë ¥ì½”ë“œ_str).to_dict()

        # OpenAI LLM ëª¨ë¸ ì´ˆê¸°í™”
        _llm_model = ChatOpenAI(model_name=LLM_MODEL, temperature=0.1, openai_api_key=openai_api_key)

        return True, "ì´ˆê¸°í™” ì„±ê³µ"

    except Exception as e:
        return False, f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"

# --- ì œê³µí•´ì£¼ì‹  í•µì‹¬ ë¡œì§ (í•¨ìˆ˜ í˜•íƒœë¡œ ì¬êµ¬ì„±) ---
# (ì£¼ì„ ì¶”ê°€ ë° ì¼ë¶€ ë¡œì§ì„ ëª…í™•í•˜ê²Œ ìˆ˜ì •)

# ... (ì œê³µí•´ì£¼ì‹  _short_doc_from_row, _keyword_search ë“±ì˜ í—¬í¼ í•¨ìˆ˜ë“¤ ìœ„ì¹˜) ...
# (ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ì–´ ìƒëµ. ì›ë³¸ ì½”ë“œì˜ í—¬í¼ í•¨ìˆ˜ë“¤ì„ ì´ ìë¦¬ì— ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.)
# IMPORTANT: ì•„ë˜ëŠ” ì œê³µëœ ì½”ë“œì˜ í•µì‹¬ ë¡œì§ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
# _short_doc_from_row, _keyword_search, _keyword_search_on_docs,
# _similarity_topk_for_term, _get_term_info_via_llm, 
# ê·¸ë¦¬ê³  search_classification_codes í•¨ìˆ˜ì™€ classification_chain_single ì²´ì¸ ì •ì˜

# (í¸ì˜ìƒ ì œê³µëœ ì½”ë“œ ì „ì²´ë¥¼ ì•„ë˜ì— í¬í•¨ì‹œì¼°ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ëª¨ë“ˆí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
def _short_doc_from_row(row: pd.Series) -> Document:
    source = row.get('ì¶œì²˜', 'í•­ëª©ë¶„ë¥˜ì§‘')
    source_info = f"ì¶œì²˜: {source}\n"
    core_fields_order = [col for col in ["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "í•­ëª©ë¶„ë¥˜ë‚´ìš©", "ì²˜ë¦¬ì½”ë“œ", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"] if col in row.index]
    core_lines = [f"{col}: {str(row[col])}" for col in core_fields_order]
    page = source_info + "\n".join(core_lines)
    meta = row.to_dict()
    return Document(page_content=page, metadata=meta)

def _keyword_search(df: pd.DataFrame, term: str) -> List[Document]:
    REQUIRED_COLS = ["í•­ëª©ëª…", "ì…ë ¥ì½”ë“œ", "ì²˜ë¦¬ì½”ë“œ", "í•­ëª©ë¶„ë¥˜ë‚´ìš©",  "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"]
    if df is None: return []
    df_copy = df.copy()
    for c in REQUIRED_COLS:
        if c in df_copy.columns: df_copy[c] = df_copy[c].astype(str)
    mask = (
        df_copy["í•­ëª©ë¶„ë¥˜ë‚´ìš©"].str.contains(term, case=False, na=False) |
        df_copy["í•­ëª©ëª…"].str.contains(term, case=False, na=False) |
        df_copy["í¬í•¨í•­ëª©"].str.contains(term, case=False, na=False) |
        df_copy["ì œì™¸í•­ëª©"].str.contains(term, case=False, na=False)
    )
    sub = df_copy.loc[mask].drop_duplicates(subset=["_rowid"], keep="first")
    return [_short_doc_from_row(r) for _, r in sub.iterrows()]

def _keyword_search_on_docs(docs: List[Document], term: str) -> List[Document]:
    if not docs: return []
    return [doc for doc in docs if term.lower() in doc.page_content.lower()]

def _similarity_topk_for_term(vs: FAISS, embeddings: OpenAIEmbeddings, term: str, k: int) -> List[Document]:
    if vs is None or embeddings is None: return []
    retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": k, "fetch_k": 30, "lambda_mult": 0.5})
    return retriever.invoke(term)

def _get_term_info_via_llm(llm: ChatOpenAI, user_query: str, num_related_terms: int) -> List[Dict[str, Any]]:
    prompt = f"""ë„ˆëŠ” ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ 'product_name' ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  í’ˆëª©ëª…ì„ ë¶„ì„í•˜ê³ , ì˜¤íƒˆìë¥¼ êµì •í•œ ë’¤ ê²€ìƒ‰ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ AIì´ë‹¤. ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì•„ë˜ "ì¶œë ¥ ì˜ˆì‹œ"ì™€ ì™„ë²½í•˜ê²Œ ë™ì¼í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•œë‹¤.
## ì…ë ¥ ì¿¼ë¦¬ ì˜ˆì‹œ:
'''product_name = ['ìŠ¤íƒ€ë²…ìŠ¤ì¡°ê°ì¼€ìµ', 'ì „ê¸°ì°¨ ì¶©ì „', 'ìƒ‰ì§€í”¼í‹°'] ...'''
### ë„ˆì˜ JSON ì¶œë ¥ ì˜ˆì‹œ:
```json
{{
  "terms": [
    {{
      "original_term": "ìŠ¤íƒ€ë²…ìŠ¤ì¡°ê°ì¼€ìµ", "term": "ìŠ¤íƒ€ë²…ìŠ¤ ì¡°ê°ì¼€ìµ", "description": "ìŠ¤íƒ€ë²…ìŠ¤ ì»¤í”¼ ì „ë¬¸ì ì—ì„œ íŒë§¤í•˜ëŠ” ì¡°ê° í˜•íƒœì˜ ì¼€ì´í¬ì…ë‹ˆë‹¤. ì»¤í”¼ì™€ í•¨ê»˜ ì¦ê¸°ëŠ” ëŒ€í‘œì ì¸ ë””ì €íŠ¸ ë©”ë‰´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.", "related_terms": ["ìŠ¤íƒ€ë²…ìŠ¤", "ì¼€ì´í¬", "ë””ì €íŠ¸"]
    }},
    {{
      "original_term": "ì „ê¸°ì°¨ ì¶©ì „", "term": "ì „ê¸°ì°¨ ì¶©ì „", "description": "ì „ê¸° ìë™ì°¨ì˜ ë°°í„°ë¦¬ì— ì „ë ¥ì„ ê³µê¸‰í•˜ëŠ” í–‰ìœ„ì…ë‹ˆë‹¤. ìë™ì°¨ë¥¼ ìš´í–‰í•˜ê¸° ìœ„í•œ ì—ë„ˆì§€ë¥¼ ì±„ìš°ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.", "related_terms": ["ì „ê¸°ì°¨", "ì¶©ì „ì†Œ", "ì—°ë£Œ"]
    }},
    {{
      "original_term": "ìƒ‰ì§€í”¼í‹°", "term": "ì±—ì§€í”¼í‹°", "description": "OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.", "related_terms": ["ìƒì„±í˜•AI", "AI", "LLM"]
    }}
  ]
}}

ì´ì œ ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•´ë¼. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ê³  JSONë§Œ ì¶œë ¥í•´ë¼. ê´€ë ¨ ìš©ì–´ëŠ” {num_related_terms}ê°œ ì¶”ì¶œí•´ë¼.
ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬: {user_query}"""
try:
res = llm.invoke(prompt)
text_content = res.content.strip()
json_match = re.search(r'{.*}', text_content, re.DOTALL)
if not json_match: raise json.JSONDecodeError("No JSON found", text_content, 0)
data = json.loads(json_match.group(0))
return data.get("terms", [])
except Exception as e:
print(f"ERROR[llm]: Failed to get term info: {e}")
return [{"term": user_query, "description": "", "related_terms": []}]
def search_classification_codes(user_query: str, all_docs_from_vs: Dict[str, List[Document]]) -> Dict[str, Any]:
if not all([_df is not None, _embeddings, _vectorstores, _llm_model]):
return {"context_docs": [], "error": "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

extracted_terms_info = _get_term_info_via_llm(_llm_model, user_query, num_related_terms=NUM_RELATED_TERMS)
all_relevant_docs, seen_docs_page_content = [], set()

for item in extracted_terms_info:
    term = item["term"]
    terms_to_search = [term] + item.get("related_terms", [])
    # Keyword Search (CSV)
    for search_term in terms_to_search:
        for doc in _keyword_search(_df, search_term):
            if doc.page_content not in seen_docs_page_content:
                all_relevant_docs.append(doc)
                seen_docs_page_content.add(doc.page_content)
    # Keyword Search (Vectorstores)
    for vs_name, doc_list in all_docs_from_vs.items():
        for search_term in terms_to_search:
            for doc in _keyword_search_on_docs(doc_list, search_term):
                new_doc = Document(page_content=f"ì¶œì²˜: {vs_name}\n{doc.page_content}", metadata=doc.metadata)
                if new_doc.page_content not in seen_docs_page_content:
                    all_relevant_docs.append(new_doc)
                    seen_docs_page_content.add(new_doc.page_content)
    # Similarity Search (Vectorstores)
    if item["description"]:
        for vs_name, vs in _vectorstores.items():
            for doc in _similarity_topk_for_term(vs, _embeddings, item["description"], k=SIMILARITY_TOP_K):
                new_doc = Document(page_content=f"ì¶œì²˜: {vs_name}\n{doc.page_content}", metadata=doc.metadata)
                if new_doc.page_content not in seen_docs_page_content:
                    all_relevant_docs.append(new_doc)
                    seen_docs_page_content.add(new_doc.page_content)

return {"context_docs": all_relevant_docs, "extracted_terms_info": extracted_terms_info}

def get_classification_report(image_bytes: bytes, openai_api_key: str, genai_api_key: str) -> str:
"""
ë©”ì¸ ë¡œì§ ì‹¤í–‰ í•¨ìˆ˜: ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ìµœì¢… ë¶„ë¥˜ ë³´ê³ ì„œ(Markdown)ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
"""
# --- 1. Geminië¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë‚´ì—­ ì¶”ì¶œ ---
try:
genai.configure(api_key=genai_api_key)
gemini_model = genai.GenerativeModel(GEMINI_MODEL)
prompt = """ê°€ê³„ë¶€ ì‚¬ì§„ì—ì„œ í‘œë¥¼ ì¸ì‹í•´ì„œ ê° í–‰ì˜ 1) í’ˆëª©ëª…(= 'ìˆ˜ì…ì¢…ë¥˜ ë° ì§€ì¶œì˜ í’ˆëª…ê³¼ ìš©ë„' ì—´), 2) ìˆ˜ì… ê¸ˆì•¡, 3) ì§€ì¶œ ê¸ˆì•¡ì„ ì¶”ì¶œí•˜ë¼.
ê·œì¹™: ê¸ˆì•¡ì˜ ì‰¼í‘œ(,)ëŠ” ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ. ê°’ì´ ë¹„ì–´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ. ì œëª©í–‰Â·ì²´í¬ë°•ìŠ¤Â·ë¹ˆì¤„ì€ ì œì™¸. ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥.
JSON ìŠ¤í‚¤ë§ˆ: {"items": [{"name": "í’ˆëª©ëª…", "income": 0, "expense": 0}, ...]}"""

    resp = gemini_model.generate_content([prompt, {'mime_type': 'image/jpeg', 'data': image_bytes}],
                                       generation_config={"response_mime_type": "application/json"})
    
    data = json.loads(resp.text)
    items = data.get("items", [])
    if not items:
        return "### ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼\n\nì´ë¯¸ì§€ì—ì„œ ê°€ê³„ë¶€ ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”."

    product_name_list = [str(it.get("name","")).strip() for it in items]
    income_list = [int(str(it.get("income", 0)).replace(",", "")) for it in items]
    expense_list = [int(str(it.get("expense", 0)).replace(",", "")) for it in items]

except Exception as e:
    return f"### ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜\n\nì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- 2. ê°œë³„ í’ˆëª© ì²˜ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘ ---
prompt_template_single = PromptTemplate.from_template("""SYSTEM: ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ 'ì…ë ¥ì½”ë“œ'ì™€ 'í•­ëª©ëª…'ì„ ì¶”ë¡ í•˜ëŠ”, ê·¹ë„ë¡œ ê¼¼ê¼¼í•˜ê³  ê·œì¹™ì„ ì—„ìˆ˜í•˜ëŠ” ë°ì´í„° ë¶„ë¥˜ AIì´ë©°, ë‹¹ì‹ ì˜ ì´ë¦„ì€ "ì¹´í…Œê³ ë¯¸(CateGOMe)"ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

[ì ˆëŒ€ ê·œì¹™ 1] expense > 0 ì´ë©´ input_code >= 1000, income > 0 ì´ë©´ input_code < 1000 ì…ë‹ˆë‹¤.
[ì ˆëŒ€ ê·œì¹™ 2] 'ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘' ì •ë³´ëŠ” 'ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘' ì •ë³´ë³´ë‹¤ í•­ìƒ ìš°ì„ í•©ë‹ˆë‹¤.
[ë¶„ë¥˜ íƒ€ì…] 90% ì´ìƒ í™•ì‹ í•  ìˆ˜ ìˆìœ¼ë©´ 'DEFINITE', í›„ë³´ê°€ ì—¬ëŸ¬ ê°œì´ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ 'AMBIGUOUS'ë¡œ ê²°ì •í•˜ì„¸ìš”. (ì˜ˆ: 'ë„¤ì´ë²„' -> ì„œë¹„ìŠ¤ê°€ ë‹¤ì–‘í•´ ëª¨í˜¸í•¨)
[ì¶œë ¥ í˜•ì‹] ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ 'DEFINITE' ë˜ëŠ” 'AMBIGUOUS' í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
(ìì„¸í•œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ì›ë³¸ì„ ë”°ë¥´ë˜, ê°„ê²°í•˜ê²Œ ìš”ì•½)
HUMAN: #Question: {question}\n#Context: {context}\nAnswer:""")

classification_chain_single = ({"question": itemgetter("question"), "context": itemgetter("context")} | prompt_template_single | _llm_model | StrOutputParser())

definite_results, ambiguous_results, failed_results = [], [], []
all_docs_from_vs = {name: list(vs.docstore._dict.values()) for name, vs in _vectorstores.items()}

for i, product_name_original in enumerate(product_name_list):
    q_single = f"product_name = ['{product_name_original}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
    search_output = search_classification_codes(q_single, all_docs_from_vs)
    
    try:
        product_name_corrected = search_output["extracted_terms_info"][0]["term"]
    except (IndexError, KeyError):
        product_name_corrected = product_name_original
    
    if "error" in search_output or not search_output["context_docs"]:
        failed_results.append({"í’ˆëª©ëª…": product_name_corrected, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": "ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨"})
        continue

    context = "\n\n---\n\n".join([doc.page_content for doc in search_output["context_docs"]]).replace("ì¶œì²˜: cases", "ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘").replace("ì¶œì²˜: classification", "ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘")
    final_question = f"product_name = ['{product_name_corrected}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
    
    try:
        output_json_str = classification_chain_single.invoke({"question": final_question, "context": context})
        json_match = re.search(r'\{.*\}', output_json_str, re.DOTALL)
        llm_result = json.loads(json_match.group(0))
        
        if llm_result.get("classification_type") == "DEFINITE":
            res = llm_result.get("result", {})
            code = str(res.get("input_code", "N/A")).strip()
            definite_results.append({"í’ˆëª©ëª…": product_name_corrected, "ì…ë ¥ì½”ë“œ": code, "í•­ëª©ëª…": _code_to_name_map.get(code, "í•­ëª©ëª… ì—†ìŒ"), "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹ ë¢°ë„": res.get("confidence", "N/A"), "ì¶”ë¡  ì´ìœ ": res.get("reason", "N/A"), "ê·¼ê±° ì •ë³´": res.get("evidence", "N/A")})
        else: # AMBIGUOUS
            for cand in llm_result.get("candidates", []):
                cand_code = str(cand.get("input_code", "")).strip()
                cand['í•­ëª©ëª…'] = _code_to_name_map.get(cand_code, "í•­ëª©ëª… ì—†ìŒ")
            ambiguous_results.append({"í’ˆëª©ëª…": product_name_corrected, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ëª¨í˜¸ì„± ì´ìœ ": llm_result.get("reason_for_ambiguity", "N/A"), "í›„ë³´": llm_result.get("candidates", []), "ê·¼ê±° ì •ë³´": llm_result.get("evidence", "N/A")})
    except Exception as e:
        failed_results.append({"í’ˆëª©ëª…": product_name_corrected, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": f"LLM ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"})

# --- 3. ìµœì¢… ë³´ê³ ì„œ(Markdown) ìƒì„± ---
# ... (ì œê³µí•´ì£¼ì‹  ë³´ê³ ì„œ ìƒì„± ë¡œì§ì„ ì—¬ê¸°ì— êµ¬í˜„) ...
# (ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ì–´ ìƒëµ. ì›ë³¸ ì½”ë“œì˜ ë³´ê³ ì„œ ìƒì„± ë¶€ë¶„ì„ ì´ ìë¦¬ì— ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.)
report = ["## ğŸ“Š ì¹´í…Œê³ ë¯¸ ë¶„ë¥˜ ê²°ê³¼ ë³´ê³ ì„œ"]
# Part 1
report.append("\n### 1. ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©\n")
if definite_results:
    df_definite = pd.DataFrame(definite_results)
    report.append("#### í’ˆëª©ë³„ ë¶„ë¥˜ ê²°ê³¼")
    report.append(df_definite[["í’ˆëª©ëª…", "ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ìˆ˜ì…", "ì§€ì¶œ"]].to_markdown(index=False))
    
    df_summary = df_definite[pd.to_numeric(df_definite['ì…ë ¥ì½”ë“œ'], errors='coerce').notna()].copy()
    if not df_summary.empty:
        df_summary['ì…ë ¥ì½”ë“œ'] = df_summary['ì…ë ¥ì½”ë“œ'].astype(int)
        df_summary_agg = df_summary.groupby('ì…ë ¥ì½”ë“œ').agg(í•­ëª©ëª…=('í•­ëª©ëª…', 'first'), ìˆ˜ì…í•©ê³„=('ìˆ˜ì…', 'sum'), ì§€ì¶œí•©ê³„=('ì§€ì¶œ', 'sum'), í•´ë‹¹í’ˆëª©ëª…=('í’ˆëª©ëª…', lambda x: ', '.join(x))).reset_index()
        report.append("\n#### ì…ë ¥ì½”ë“œë³„ ìš”ì•½ ê²°ê³¼")
        report.append(df_summary_agg.to_markdown(index=False))
else:
    report.append("ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

# Part 2
if ambiguous_results:
    report.append("\n\n### 2. ì‚¬ìš©ìì˜ ê²€í† ê°€ í•„ìš”í•œ í’ˆëª©\n")
    report.append("> ì•„ë˜ í’ˆëª©ë“¤ì€ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹¨ì¼ ì½”ë“œë¥¼ í™•ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œì‹œëœ í›„ë³´ì™€ ì´ìœ ë¥¼ í™•ì¸ í›„ ì§ì ‘ ì½”ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\n")
    for res in ambiguous_results:
        report.append(f"---\n**í’ˆëª©ëª…: {res['í’ˆëª©ëª…']}** (ìˆ˜ì…: {res['ìˆ˜ì…']:,}ì›, ì§€ì¶œ: {res['ì§€ì¶œ']:,}ì›)")
        report.append(f"**- ê²€í†  í•„ìš” ì´ìœ :** {res['ëª¨í˜¸ì„± ì´ìœ ']}")
        if res['í›„ë³´']:
             df_cand = pd.DataFrame(res['í›„ë³´']).rename(columns={'input_code': 'ì…ë ¥ì½”ë“œ', 'confidence': 'ì‹ ë¢°ë„', 'reason': 'ì´ìœ '})
             report.append(df_cand[['ì…ë ¥ì½”ë“œ', 'í•­ëª©ëª…', 'ì‹ ë¢°ë„', 'ì´ìœ ']].to_markdown(index=False))

# Part 3
if definite_results:
    report.append("\n\n### 3. ëª…í™•í•œ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±°\n")
    for res in definite_results:
        report.append(f"---\n**í’ˆëª©ëª…: {res['í’ˆëª©ëª…']} (ì„ íƒëœ ì½”ë“œ: {res['ì…ë ¥ì½”ë“œ']})**")
        report.append(f"**- ì¶”ë¡  ì´ìœ :** {res['ì¶”ë¡  ì´ìœ ']}")
        report.append(f"**- í•µì‹¬ ê·¼ê±°:**\n```\n{res['ê·¼ê±° ì •ë³´']}\n```")

# Part 4
if failed_results:
    report.append("\n\n### 4. ì²˜ë¦¬ ì‹¤íŒ¨ í•­ëª©\n")
    report.append(pd.DataFrame(failed_results).to_markdown(index=False))

return "\n".join(report)

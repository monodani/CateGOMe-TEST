# ========================================
# ğŸ”§ ì„¤ì •ê°’
# ========================================
import streamlit as st

# API Key ì„¤ì • (Streamlit secrets ì‚¬ìš©)
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GENAI_API_KEY = st.secrets["GENAI_API_KEY"]

# --- Global (1íšŒ ë¡œë“œ ìºì‹œ) ----------------------------------------------------
EMBED_MODEL = "text-embedding-3-large"
LLM_MODEL = "gpt-4o"  # í†µí•© ëª¨ë¸ëª… ë³€ìˆ˜ ì‚¬ìš©

VECTORSTORE_DIR_CASES = "vectorstores/cases"
INDEX_NAME_CASES = "cases_index"
VECTORSTORE_DIR_CLASSIFICATION = "vectorstores/classification"
INDEX_NAME_CLASSIFICATION = "classification_index"
CSV_PATH = "data/classification_code.csv"

REQUIRED_COLS = ["í•­ëª©ëª…", "ì…ë ¥ì½”ë“œ", "ì²˜ë¦¬ì½”ë“œ", "í•­ëª©ë¶„ë¥˜ë‚´ìš©", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"]


# ========================================
# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ========================================
import os
import re
import json
import ast
from typing import List, Dict, Any
from operator import itemgetter

import pandas as pd
from PIL import Image
import google.generativeai as genai

from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

import chardet
import base64

# Gemini ì„¤ì •
genai.configure(api_key=GENAI_API_KEY)

# ========================================
# Streamlit í˜ì´ì§€ ì„¤ì •
# ========================================
try:
    icon = Image.open("assets/CateGOMe_logo.png")
except FileNotFoundError:
    icon = "ğŸ»"  # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ ì´ëª¨ì§€ë¡œ ëŒ€ì²´

st.set_page_config(
    page_title="ì¹´í…Œê³ ë¯¸-í†µê³„ì²­ í•­ëª©ìë™ë¶„ë¥˜AI",
    page_icon=icon,
    layout="wide"
)

# ========================================
# Colab ì´ˆê¸°í™” ì½”ë“œ ê·¸ëŒ€ë¡œ (ìºì‹± ì¶”ê°€)
# ========================================
@st.cache_resource
def initialize_system():
    try:
        _embeddings = OpenAIEmbeddings(model=EMBED_MODEL, openai_api_key=OPENAI_API_KEY)

        # === ìˆ˜ì •ëœ ë¶€ë¶„: ë‘ ê°œì˜ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ===
        _vectorstore_cases = FAISS.load_local(
            folder_path=VECTORSTORE_DIR_CASES,
            embeddings=_embeddings,
            index_name=INDEX_NAME_CASES,
            allow_dangerous_deserialization=True
        )

        _vectorstore_classification = FAISS.load_local(
            folder_path=VECTORSTORE_DIR_CLASSIFICATION,
            embeddings=_embeddings,
            index_name=INDEX_NAME_CLASSIFICATION,
            allow_dangerous_deserialization=True
        )

        # ë²¡í„°ìŠ¤í† ì–´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬í•˜ì—¬ í™•ì¥ì„± í™•ë³´
        _vectorstores = {
            "cases": _vectorstore_cases,
            "classification": _vectorstore_classification
        }
        # ============================================

        # CSV ì¸ì½”ë”© ê°ì§€ í›„ ì½ê¸°
        with open(CSV_PATH, 'rb') as f:
            result = chardet.detect(f.read())
            encoding = result['encoding']
        
        _df = pd.read_csv(CSV_PATH, encoding=encoding, dtype={'ì…ë ¥ì½”ë“œ':str})
        missing = [c for c in REQUIRED_COLS if c not in _df.columns]
        if missing:
            raise KeyError(f"ERROR[csv]: Missing required columns: {missing}")
        # ê³ ìœ í‚¤(ì¤‘ë³µ ì œê±°ìš©) ì—†ìœ¼ë©´ í–‰ ì¸ë±ìŠ¤ ì‚¬ìš©
        _df = _df.reset_index(drop=False).rename(columns={"index": "_rowid"})

        # LLM ëª¨ë¸ë„ ìºì‹œ
        _llm_model = ChatOpenAI(
            model_name=LLM_MODEL,
            temperature=0.0,
            openai_api_key=OPENAI_API_KEY
        )

        return _embeddings, _vectorstores, _df, _llm_model

    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None, None, None

# ì´ˆê¸°í™”
_embeddings, _vectorstores, _df, _llm_model = initialize_system()

# ========================================
# Colab í—¬í¼ í•¨ìˆ˜ë“¤ ê·¸ëŒ€ë¡œ
# ========================================

        
def _short_doc_from_row(row: pd.Series) -> Document:
    """
    page_contentëŠ” í† í° ë‚­ë¹„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•µì‹¬ í•„ë“œë§Œ.
    ë‚˜ë¨¸ì§€ëŠ” metadataì— ë‹´ëŠ”ë‹¤.
    'ì¶œì²˜' ì •ë³´ë¥¼ page_content ë§¨ ì•ì— ì¶”ê°€í•˜ê³ , 'ì…ë ¥ì½”ë“œ'ë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    source = row.get('ì¶œì²˜', 'í•­ëª©ë¶„ë¥˜ì§‘')
    source_info = f"ì¶œì²˜: {source}\n"

    core_fields_order = [col for col in ["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "í•­ëª©ë¶„ë¥˜ë‚´ìš©", "ì²˜ë¦¬ì½”ë“œ", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©", "ì¶œì²˜"] if col in row.index]

    core_lines = []
    for col in core_fields_order:
        value = row[col]

        # 'ì…ë ¥ì½”ë“œ' ì»¬ëŸ¼ì¼ ê²½ìš°, ì •ìˆ˜ë¡œ ë³€í™˜ì„ ì‹œë„
        if col == "ì…ë ¥ì½”ë“œ":
            value_str = str(value).strip()
        else:
            value_str = str(value)
        # ============================

        core_lines.append(f"{col}: {value_str}")

    page = source_info + "\n".join(core_lines)
    meta = row.to_dict()
    return Document(page_content=page, metadata=meta)


def _keyword_search(df: pd.DataFrame, term: str) -> List[Document]:
    """ë¶€ë¶„ì¼ì¹˜ contains, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ. ìƒí•œ ì—†ìŒ(ìš”ì²­ ë°˜ì˜)."""
    if df is None:  # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        return []
    # NaN ì•ˆì „ ì²˜ë¦¬ ë° íƒ€ì… ë³€í™˜
    df_copy = df.copy()  # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³€ê²½ ë°©ì§€
    for c in REQUIRED_COLS:
        if c in df_copy.columns and df_copy[c].dtype != object:
            df_copy[c] = df_copy[c].astype(str)

    mask = (
        df_copy["í•­ëª©ë¶„ë¥˜ë‚´ìš©"].str.contains(term, case=False, na=False) |
        df_copy["í•­ëª©ëª…"].str.contains(term, case=False, na=False) |
        df_copy["í¬í•¨í•­ëª©"].str.contains(term, case=False, na=False) |
        df_copy["ì œì™¸í•­ëª©"].str.contains(term, case=False, na=False)
    )
    sub = df_copy.loc[mask]
    # ì¤‘ë³µ ì œê±°(í–‰ ì¸ë±ìŠ¤ ê¸°ë°˜)
    sub = sub.drop_duplicates(subset=["_rowid"], keep="first")
    return [_short_doc_from_row(r) for _, r in sub.iterrows()]

def create_extended_code_map(df):
    """ë²”ìœ„í˜• ì½”ë“œë¥¼ ê°œë³„ ì½”ë“œë¡œ í™•ì¥í•˜ì—¬ ë§¤í•‘ (ìš°ì„ ìˆœìœ„: ì´ì‚°í˜• > ë²”ìœ„í˜•)"""
    code_map = {}
    
    # 1ë‹¨ê³„: ë²”ìœ„í˜• ë¨¼ì € ì²˜ë¦¬ (ë‚®ì€ ìš°ì„ ìˆœìœ„)
    for _, row in df[df['ì…ë ¥ì½”ë“œ'].str.contains('-', na=False)].iterrows():
        input_code = str(row['ì…ë ¥ì½”ë“œ']).strip()
        item_name = row['í•­ëª©ëª…']
        
        parts = input_code.split('-')
        if len(parts) == 2:
            try:
                start_num = int(parts[0].strip())
                end_num = int(parts[1].strip())
                
                # ë²”ìœ„ ë‚´ ëª¨ë“  ì½”ë“œë¥¼ ë§¤í•‘
                for num in range(start_num, end_num + 1):
                    code_str = f"{num:04d}"
                    code_map[code_str] = item_name
            except:
                pass
        
        # ë²”ìœ„ í‘œí˜„ ìì²´ë„ ì €ì¥
        code_map[input_code] = item_name
    
    # 2ë‹¨ê³„: ì´ì‚°í˜• ë‚˜ì¤‘ì— ì²˜ë¦¬ (ë†’ì€ ìš°ì„ ìˆœìœ„ - ë®ì–´ì“°ê¸°)
    for _, row in df[~df['ì…ë ¥ì½”ë“œ'].str.contains('-', na=False)].iterrows():
        input_code = str(row['ì…ë ¥ì½”ë“œ']).strip()
        item_name = row['í•­ëª©ëª…']
        
        # ì´ì‚°í˜•ì€ ë¬´ì¡°ê±´ ë®ì–´ì“°ê¸° (ê°™ì€ ì½”ë“œ ì—¬ëŸ¬ í–‰ ìˆì–´ë„ ê°™ì€ í•­ëª©ëª…ì´ë¯€ë¡œ OK)
        code_map[input_code] = item_name
    
    return code_map
    


def _keyword_search_on_docs(docs: List[Document], term: str) -> List[Document]:
    """ë©”ëª¨ë¦¬ì— ë¡œë“œëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if not docs:
        return []

    # page_contentì— termì´ í¬í•¨ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    return [doc for doc in docs if term.lower() in doc.page_content.lower()]


def _similarity_topk_for_term(vs: FAISS, embeddings: OpenAIEmbeddings, term: str, k: int = 5) -> List[Document]:
    if vs is None or embeddings is None:  # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        return []
    retriever = vs.as_retriever(
        search_type="mmr",  # MMR ì‚¬ìš© ìœ ì§€
        search_kwargs={"k": k, "fetch_k": 30, "lambda_mult": 0.5}
    )
    return retriever.invoke(term)

def _get_term_info_via_llm(llm: ChatOpenAI, user_query: str, num_related_terms: int = 3) -> List[Dict[str, Any]]:
    """
    LLMì„ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í’ˆëª©ëª…ë“¤ì„ ì¶”ì¶œí•˜ê³ , ê° í’ˆëª©ëª…ì— ëŒ€í•œ ì„¤ëª…ê³¼ ê´€ë ¨ ìš©ì–´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
    ì•ˆì •ì ì¸ JSON ì¶”ì¶œì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ì™€ íŒŒì‹± ë¡œì§ì´ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤.
    """
    if llm is None:
        return []

    # ì´ í•¨ìˆ˜ì—ì„œë§Œ gpt-4o ëª¨ë¸ ì‚¬ìš©
    gpt_llm = ChatOpenAI(
        model_name="gpt-5-mini",
        temperature=0.0,
        openai_api_key=OPENAI_API_KEY
    )
    
    # === í’ˆëª© ì„¤ëª… ë° ê´€ë ¨ì–´ ë°˜í™˜ í”„ë¡¬í”„íŠ¸ ===
    prompt = f"""
ë„ˆëŠ” **ì‚¬ìš©ìì˜ ê°€ê³„ë¶€ì—ì„œ ì¶”ì¶œëœ ì •ë³´**ë¡œ êµ¬ì„±ëœ ì¿¼ë¦¬ì—ì„œ 'product_name' ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  í’ˆëª©ëª…ì„ ë¶„ì„í•˜ê³ , ì˜¤íƒˆìë¥¼ êµì •í•œ ë’¤ ê²€ìƒ‰ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ AIì´ë‹¤.
**ì¿¼ë¦¬ì— í¬í•¨ëœ ì •ë³´ì˜ ì¶œì²˜ê°€ ê°€ê³„ë¶€**ë¼ëŠ” ê²ƒì„ ë°˜ë“œì‹œ ìœ ë…í•´ì„œ **ê°€ê³„ë¶€ì˜ ìˆ˜ì…, ì§€ì¶œ í•­ëª©ì— ëŒ€í•œ ê²ƒì„ì„ ê³ ë ¤í•˜ì—¬** ì•„ë˜ì˜ ì‘ì—…ì ˆì°¨ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•œë‹¤.

## ì‘ì—… ì ˆì°¨ (ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ë”°ë¥¼ ê²ƒ) ##
1. **í’ˆëª©ëª… ì¶”ì¶œ:** `product_name = [...]` ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ëª¨ë“  ì›ë³¸ í’ˆëª©ëª…ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œí•œë‹¤.
2. **ì˜¤íƒˆì êµì •:** ê° ì›ë³¸ í’ˆëª©ëª…ì˜ ì˜¤íƒˆìë‚˜ ë¶ˆë¶„ëª…í•œ í‘œí˜„ì„ ê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  ì¼ë°˜ì ì¸ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•œë‹¤(ì˜ˆ: "ìƒ‰ì§€í”¼í‹°" -> "ì±—ì§€í”¼í‹°", "íŒŒí”Œë¦¬ì‹œí‹°" -> "í¼í”Œë ‰ì‹œí‹°", "ìˆ˜ì°¨ë¹„" -> "ì£¼ì°¨ë¹„" ë“±). ë§Œì•½ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ë‹¤ë©´ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.
3. **ì„¤ëª… ìƒì„±:** **ìˆ˜ì •ëœ í’ˆëª©ëª…**ì— ëŒ€í•´, ê·¸ í’ˆëª©ì˜ ë³¸ì§ˆê³¼ ëª©ì ì„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•œë‹¤.
4. **ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ (ë§¤ìš° ì¤‘ìš”):**
   - **ìˆ˜ì •ëœ í’ˆëª©ëª…**ê³¼ **ë„¤ê°€ ì‘ì„±í•œ ì„¤ëª…**ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬, ê²€ìƒ‰ì— ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” í•µì‹¬ ê´€ë ¨ ìš©ì–´ë¥¼ {num_related_terms}ê°œ ì¶”ì¶œí•œë‹¤.
   - **ë‹¨ìˆœ ë™ì˜ì–´ë¥¼ ë„˜ì–´, ê·¸ í’ˆëª©ì˜ 'ëª©ì 'ì´ë‚˜ 'ìƒìœ„ ì¹´í…Œê³ ë¦¬'ì— í•´ë‹¹í•˜ëŠ” ê°œë…ì ì¸ ë‹¨ì–´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•œë‹¤.** (ì˜ˆ: 'ìˆ˜ì†Œì°¨/ì „ê¸°ì°¨ ì¶©ì „'ì˜ ëª©ì ì€ 'ì—°ë£Œ'ë¥¼ ì±„ìš°ëŠ” ê²ƒì´ë¯€ë¡œ 'ì—°ë£Œ'ë‚˜ 'ì—ë„ˆì§€'ë¥¼ í¬í•¨)
   - **ë„ì–´ì“°ê¸°ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” í•œ ë‹¨ì–´ í˜•íƒœì—¬ì•¼ í•œë‹¤.** (ì˜ˆ: "ì—°ë£Œ", "ì—ë„ˆì§€", êµ¬ë…ì„œë¹„ìŠ¤" ë“± )

5. **JSON ì¶œë ¥:** ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì•„ë˜ "ì¶œë ¥ ì˜ˆì‹œ"ì™€ **ì™„ë²½í•˜ê²Œ ë™ì¼í•œ JSON í˜•ì‹**ìœ¼ë¡œë§Œ ì‘ë‹µí•œë‹¤.

## ì…ë ¥ ë° ì¶œë ¥ ì˜ˆì‹œ (ë§¤ìš° ì¤‘ìš”) ##
### ì…ë ¥ ì¿¼ë¦¬ ì˜ˆì‹œ:
'''product_name = ['ëª¨ë‘ ìŒˆ', 'ìˆ˜ì†Œì°¨ ì¶©ì „', 'íŒŒí”Œë¦¬ì‹œí‹°'] ...'''

### ë„ˆì˜ JSON ì¶œë ¥ ì˜ˆì‹œ:
```json
{{
  "terms": [
    {{
      "original_term": "ëª¨ë‘ ìŒˆ",
      "term": "ëª¨ë‘ ìŒˆ",
      "description": "ëª¨ë‘ ìŒˆì€ ìƒì¶”, ê¹»ì ë“± ë‹¤ì–‘í•œ ì±„ì†Œì™€ ë°¥Â·ê³ ê¸°Â·ì–‘ë…ì„ í•¨ê»˜ ì‹¸ë¨¹ëŠ” ì‹ì‚¬ ë©”ë‰´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.",
      "related_terms": ["ì‹ì‚¬", "ì±„ì†Œ", "ë°˜ì°¬", ...]
    }},
    {{
      "original_term": "ìˆ˜ì†Œì°¨ ì¶©ì „",
      "term": "ìˆ˜ì†Œì°¨ ì¶©ì „",
      "description": "ìˆ˜ì†Œì°¨ ì¶©ì „ì€ ìˆ˜ì†Œ ì—°ë£Œì „ì§€ ì°¨ëŸ‰ì„ ìš´í–‰í•˜ê¸° ìœ„í•´ ìˆ˜ì†Œ ì—°ë£Œë¥¼ ë³´ê¸‰í•˜ëŠ” í–‰ìœ„ì…ë‹ˆë‹¤.",
      "related_terms": ["ìˆ˜ì†Œ", "ì—°ë£Œ", "ì¶©ì „", "ìˆ˜ì†Œì°¨", "ì¹œí™˜ê²½ì°¨", ...]
    }},
    {{
      "original_term": "íŒŒí”Œë¦¬ì‹œí‹°",
      "term": "í¼í”Œë ‰ì‹œí‹°",
      "description": "í¼í”Œë ‰ì‹œí‹°ëŠ” ì‹¤ì‹œê°„ ì›¹ê²€ìƒ‰ê³¼ ìƒì„±í˜• AI ì§ˆì˜ì‘ë‹µì„ ê²°í•©í•œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¡œ, AI ê²€ìƒ‰Â·ì±—ë´‡ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” êµ¬ë… ê¸°ë°˜ í”Œë«í¼ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
      "related_terms": ["êµ¬ë…ì„œë¹„ìŠ¤", "í”Œë«í¼", "AI", "ê²€ìƒ‰", ...]
    }}
  ]
}}
---
ì´ì œ ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•´ë¼. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ê³  JSONë§Œ ì¶œë ¥í•´ë¼.

ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬: {user_query}
"""

    try:
        res = gpt_llm.invoke(prompt)
        text_content = res.content.strip()

        json_match = re.search(r'\{.*\}', text_content, re.DOTALL)
        if not json_match:
            raise json.JSONDecodeError("LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", text_content, 0)

        json_str = json_match.group(0)
        data = json.loads(json_str)

        terms_info = data.get("terms", [])

        # ê¸°ë³¸ ì •ë¦¬
        cleaned_terms_info = []
        for item in terms_info:
            if item.get("term") and item.get("description"):
                cleaned_related = []
                for rt in item.get("related_terms", []):
                    rt_norm = re.sub(r"\s+", " ", str(rt)).strip().lower()
                    if rt_norm:
                        cleaned_related.append(re.sub(r"\s+", " ", str(rt)).strip())
                cleaned_terms_info.append({
                    "term": re.sub(r"\s+", " ", str(item["term"])).strip(),
                    "description": item["description"].strip(),
                    "related_terms": cleaned_related[:num_related_terms]
                })
        return cleaned_terms_info

    except Exception as e:
        # í´ë°± ë¡œì§
        return [{"term": user_query, "description": "", "related_terms": []}]

# search_classification_codes í•¨ìˆ˜ 
def search_classification_codes(
    user_query: str,
    all_docs_from_vs: Dict[str, List[Document]],  # íŒŒë¼ë¯¸í„°
    sim_topk_per_term: int = 5,  # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
    num_related_terms: int = 3  # LLM ê´€ë ¨ ìš©ì–´ ê°œìˆ˜
) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•´ ë¶„ë¥˜ ì½”ë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    (Colab ì½”ë“œ ê·¸ëŒ€ë¡œ)
    """
    # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
    if _df is None or _embeddings is None or _vectorstores is None or _llm_model is None or OPENAI_API_KEY is None:
        return {
            "query": user_query,
            "extracted_terms_info": [],
            "results": {"keyword": [], "similarity": []},
            "context_docs": [],
            "error": "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (ë°ì´í„°, ë²¡í„°ìŠ¤í† ì–´, LLM ë˜ëŠ” API í‚¤). ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        }

    if not isinstance(user_query, str) or not user_query.strip():
        return {
            "query": user_query,
            "extracted_terms_info": [],
            "results": {"keyword": [], "similarity": []},
            "context_docs": [],
            "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤."
        }

    # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª…, ê´€ë ¨ ìš©ì–´ ë°›ê¸°
    extracted_terms_info = _get_term_info_via_llm(_llm_model, user_query, num_related_terms=num_related_terms)

    all_relevant_docs: List[Document] = []  # í‚¤ì›Œë“œ ë˜ëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    seen_docs_page_content = set()  # ì¤‘ë³µ ì œê±°ìš©

    all_keyword_docs_raw: List[Document] = []  # ë””ë²„ê¹…ìš©: ì¤‘ë³µ í¬í•¨ í‚¤ì›Œë“œ ê²°ê³¼
    all_similarity_docs_raw: List[Document] = []  # ë””ë²„ê¹…ìš©: ì¤‘ë³µ í¬í•¨ ìœ ì‚¬ë„ ê²°ê³¼

    for item in extracted_terms_info:
        term = item["term"]
        description = item["description"]
        related_terms = item.get("related_terms", [])

        # 2. ê° í•µì‹¬ ìš©ì–´(ì›ì–´)ì™€ ê´€ë ¨ ìš©ì–´ì— ëŒ€í•´ í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰
        terms_to_keyword_search = [term] + related_terms
        for search_term in terms_to_keyword_search:
            kw_docs = _keyword_search(_df, search_term)
            if kw_docs:
                all_keyword_docs_raw.extend(kw_docs)  # ë””ë²„ê¹…ìš© ê²°ê³¼ ì¶”ê°€
                for doc in kw_docs:
                    if doc.page_content not in seen_docs_page_content:
                        all_relevant_docs.append(doc)
                        seen_docs_page_content.add(doc.page_content)

        # === ê²€ìƒ‰ B: ë²¡í„°ìŠ¤í† ì–´ì— ëŒ€í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ ===
        for name, doc_list in all_docs_from_vs.items():
            for search_term in terms_to_keyword_search:
                kw_docs_vs = _keyword_search_on_docs(doc_list, search_term)
                if kw_docs_vs:
                    docs_to_add = [Document(page_content=f"ì¶œì²˜: {name}\n{doc.page_content}", metadata=doc.metadata) for doc in kw_docs_vs]
                    all_keyword_docs_raw.extend(docs_to_add)
                    for doc in docs_to_add:
                        if doc.page_content not in seen_docs_page_content:
                            all_relevant_docs.append(doc)
                            seen_docs_page_content.add(doc.page_content)

        # === ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰ ===
        if description:
            for name, vs in _vectorstores.items():
                sim_docs = _similarity_topk_for_term(vs, _embeddings, description, k=sim_topk_per_term)
                if sim_docs:
                    docs_to_add = []
                    for doc in sim_docs:
                        new_doc = Document(page_content=f"ì¶œì²˜: {name}\n{doc.page_content}", metadata=doc.metadata)
                        docs_to_add.append(new_doc)

                    all_similarity_docs_raw.extend(docs_to_add)
                    for doc in docs_to_add:
                        if doc.page_content not in seen_docs_page_content:
                            all_relevant_docs.append(doc)
                            seen_docs_page_content.add(doc.page_content)

    # 4. ìˆ˜ì§‘ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±° (all_relevant_docsì— ì´ë¯¸ ì¤‘ë³µ ì œê±°ë˜ì–´ ìˆ˜ì§‘ë¨)
    unique_docs_objects = all_relevant_docs  # ë³€ìˆ˜ëª… í†µì¼

    return {
        "query": user_query,
        "extracted_terms_info": extracted_terms_info,
        "results": {
            "keyword": all_keyword_docs_raw,  # ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (ì¤‘ë³µ í¬í•¨ ê°€ëŠ¥)
            "similarity": all_similarity_docs_raw  # ëª¨ë“  ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ (ì¤‘ë³µ í¬í•¨ ê°€ëŠ¥)
        },
        "context_docs": unique_docs_objects  # GPTì— ì „ë‹¬í•  ìµœì¢… ì¤‘ë³µ ì œê±°ëœ Document ê°ì²´ ëª©ë¡
    }

# prompt_template_single
prompt_template_single = PromptTemplate.from_template("""
    SYSTEM: ë‹¹ì‹ ì€ **ê°€ê³„ë¶€ë¡œë¶€í„° ì¶”ì¶œëœ** ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ 'ì…ë ¥ì½”ë“œ'ì™€ 'í•­ëª©ëª…'ì„ ì¶”ë¡ í•˜ëŠ”, ê·¹ë„ë¡œ ê¼¼ê¼¼í•˜ê³  ê·œì¹™ì„ ì—„ìˆ˜í•˜ëŠ” ë°ì´í„° ë¶„ë¥˜ AIì´ë©°, ë‹¹ì‹ ì˜ ì´ë¦„ì€ "ì¹´í…Œê³ ë¯¸(CateGOMe)"ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

    ## ì…ë ¥ì½”ë“œ í˜•ì‹ ì°¸ê³ ì‚¬í•­ ##
    1, ì…ë ¥ì½”ë“œëŠ” ë‹¨ì¼ê°’(ì˜ˆ: "0120", "3610") ë˜ëŠ” ë²”ìœ„ê°’(ì˜ˆ: "0110-0120")ìœ¼ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    2. ë²”ìœ„ê°’ì˜ ê²½ìš°, í•´ë‹¹ ë²”ìœ„ì— í¬í•¨ë˜ëŠ” ê°œë³„ ì½”ë“œë„ ìœ íš¨í•©ë‹ˆë‹¤.
    3. ì˜ˆ: "0110-0120" ë²”ìœ„ì—ëŠ” "0110", "0111", ..., "0119", "0120"ì´ ëª¨ë‘ í¬í•¨ë©ë‹ˆë‹¤.
    4. ì•ìë¦¬ "0"ì€ ìœ ì§€í•´ì„œ ë°˜í™˜í•´ì£¼ì„¸ìš” (ì˜ˆ: "0120" ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    
    ## ì ˆëŒ€ ê·œì¹™ (ê°€ì¥ ì¤‘ìš”! ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ) ##
    1. **ìˆ˜ì…/ì§€ì¶œ ê·œì¹™:** `question`ì˜ `expense` ê°’ì´ 0ë³´ë‹¤ í¬ë©´, `input_code`ëŠ” **ì ˆëŒ€ë¡œ 1000 ë¯¸ë§Œì´ ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** ë°˜ëŒ€ë¡œ `income` ê°’ì´ 0ë³´ë‹¤ í¬ë©´, `input_code`ëŠ” **ì ˆëŒ€ë¡œ 1000 ì´ìƒì´ ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** ì˜ˆì™¸ëŠ” ì—†ìŠµë‹ˆë‹¤.
    2. **ì •ë³´ ìš°ì„ ìˆœìœ„ ê·œì¹™:** `context`ì—ì„œ `ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘`(ë˜ëŠ” cases) ì •ë³´ëŠ” `ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘` ì •ë³´ë³´ë‹¤ **í•­ìƒ ìš°ì„ **í•©ë‹ˆë‹¤. ë§Œì•½ ë‘ ì •ë³´ê°€ ì¶©ëŒí•˜ë©´, ë¬´ì¡°ê±´ 'ì¡°ì‚¬ì‚¬ë¡€ì§‘'ì˜ ì½”ë“œë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.

    ## ì‘ì—… ì ˆì°¨ ##
    1. **ì…ë ¥ ë¶„ì„:** `question`ì˜ `í’ˆëª©ëª…`, `income`, `expense` ê°’ì„ í™•ì¸í•˜ê³  [ì ˆëŒ€ ê·œì¹™ 1]ì„ ê¸°ì–µí•©ë‹ˆë‹¤.
    2. **ì»¨í…ìŠ¤íŠ¸ ë¶„ì„:**
        - `í’ˆëª©ëª…`ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” **'ì¡°ì‚¬ì‚¬ë¡€ì§‘'** ë‚´ìš©ì´ ìˆëŠ”ì§€ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
        - ë§Œì•½ ëª…í™•í•œ ì‚¬ë¡€ê°€ ìˆë‹¤ë©´, [ì ˆëŒ€ ê·œì¹™ 2]ì— ë”°ë¼ í•´ë‹¹ ì½”ë“œë¥¼ **ìµœìš°ì„  í›„ë³´**ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.
        - ëª…í™•í•œ ì‚¬ë¡€ê°€ ì—†ë‹¤ë©´, 'í•­ëª©ë¶„ë¥˜ì§‘'ì—ì„œ ê°€ì¥ ì í•©í•œ ì •ì˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    3. **ë¶„ë¥˜ íƒ€ì… ê²°ì •:**
        - **(DEFINITE ì¡°ê±´):** ìœ„ì˜ ê³¼ì •ì„ ê±°ì³, ë‹¨ í•˜ë‚˜ì˜ ì…ë ¥ì½”ë“œë¥¼ 90% ì´ìƒì˜ ì‹ ë¢°ë„ë¡œ í™•ì‹ í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì—ë§Œ "DEFINITE"ë¡œ ê²°ì •í•©ë‹ˆë‹¤. (ì˜ˆ: 'ì±—ì§€í”¼í‹°' -> 'ì±—ì§€í”¼í‹° êµ¬ë…ë£Œ' ì‚¬ë¡€ê°€ ëª…í™•íˆ ì¡´ì¬)
        - **(AMBIGUOUS ì¡°ê±´):** ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹í•˜ë©´ **ë°˜ë“œì‹œ "AMBIGUOUS"**ë¡œ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            - í’ˆëª©ëª…ì´ ë„ˆë¬´ ì¼ë°˜ì ì´ì–´ì„œ ì—¬ëŸ¬ ì½”ë“œê°€ í›„ë³´ê°€ ë  ë•Œ (ì˜ˆ: 'ê³ ë“±ì–´' -> ê°„ê³ ë“±ì–´? ë°”ë‹¤ì–´ë¥˜? ìˆ˜ì‚°ë™ë¬¼í†µì¡°ë¦¼? ì•Œ ìˆ˜ ì—†ìŒ)
            - **í’ˆëª©ëª…ì´ íŠ¹ì • íšŒì‚¬ ì´ë¦„ì´ê³ , ê·¸ íšŒì‚¬ê°€ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ìƒí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš° (ì˜ˆ: 'ë„¤ì´ë²„'  -> ì˜¨ë¼ì¸ì‡¼í•‘ëª°, í˜ì´ ê²°ì œ, ì›¹íˆ° ë“± ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ ìƒí’ˆì´ ìˆì–´ í•˜ë‚˜ë¡œ íŠ¹ì • ë¶ˆê°€)**
            - ì†Œë“ì˜ ì£¼ì²´(ê°€êµ¬ì£¼, ë°°ìš°ì, ê¸°íƒ€ê°€êµ¬ì› ë“±)ê°€ ë¶ˆëª…í™•í•˜ì—¬ ì—¬ëŸ¬ ì½”ë“œê°€ í›„ë³´ê°€ ë  ë•Œ (ì˜ˆ: 'ê¸‰ì—¬' -> ê°€êµ¬ì£¼ê¸‰ì—¬? ë°°ìš°ìê¸‰ì—¬? ê¸°íƒ€ê°€êµ¬ì›ê¸‰ì—¬?)
    4. **JSON ì¶œë ¥:**ê²°ì •ëœ ë¶„ë¥˜ íƒ€ì…ì— ë§ëŠ” JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

    ---
    ## ì¢‹ì€ ì˜ˆì‹œì™€ ë‚˜ìœ ì˜ˆì‹œ ##

    - **Question:** product_name = ['í• ë¦¬ìŠ¤ì»¤í”¼ì¡°ê°ì¼€ìµ'], expense = [10000]
    - **Context:** ... [í•­ëª©ë¶„ë¥˜ì§‘] ì¼€ì´í¬: 1085(ì¼€ì´í¬) ... [ì¡°ì‚¬ì‚¬ë¡€ì§‘] ì»¤í”¼ìˆ êµ¬ë§¤ ì¡°ê° ì¼€ìµ: 7560(ì£¼ì Â·ì»¤í”¼ìˆ) ...
    - **ë‚˜ìœ íŒë‹¨:** 'ì¼€ì´í¬'ë¼ëŠ” ì¼ë°˜ ë¶„ë¥˜ë¥¼ ë³´ê³  `1085`ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒ.
    - **ì¢‹ì€ íŒë‹¨:** [ì •ë³´ ìš°ì„ ìˆœìœ„ ê·œì¹™]ì— ë”°ë¼ 'ì¡°ì‚¬ì‚¬ë¡€ì§‘'ì˜ `7560`ì„ ì„ íƒí•˜ê³  "DEFINITE"ë¡œ ë¶„ë¥˜.

    ---
    ## ì¶œë ¥ í˜•ì‹ (ì•„ë˜ í˜•ì‹ ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ë‹µ) ##

    ### A. ëª…í™•í•œ ê²½ìš° (DEFINITE):
    ```json
    {{
      "classification_type": "DEFINITE",
      "result": {{
        "input_code": "ì¶”ë¡ í•œ ìˆ«ì ì…ë ¥ì½”ë“œ",
        "confidence": "ì‹ ë¢°ë„ (ì˜ˆ: 95%)",
        "reason": "ì ˆëŒ€ ê·œì¹™ê³¼ ì •ë³´ ìš°ì„ ìˆœìœ„ ê·œì¹™ì— ì…ê°í•˜ì—¬ ì´ ì½”ë“œë¥¼ ì„ íƒí•œ ëª…í™•í•œ ì´ìœ .(ê²½ì–´ë¡œ ë‹µë³€í•´ì•¼ í•¨.)",
        "evidence": "ê·¼ê±°ë¡œ ì‚¬ìš©í•œ ê°€ì¥ í•µì‹¬ì ì¸ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©(ì²­í¬) í•˜ë‚˜ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬"
      }}
    }}
    ```

    ### B. ëª¨í˜¸í•œ ê²½ìš° (AMBIGUOUS)
    ```json
    {{
      "classification_type": "AMBIGUOUS",
      "reason_for_ambiguity": "ì™œ ë‹¨ì¼ ì½”ë“œë¡œ í™•ì •í•  ìˆ˜ ì—†ëŠ”ì§€ì— ëŒ€í•œ í•µì‹¬ ì´ìœ  (ì˜ˆ: 'ë³´í—˜ì˜ ì¢…ë¥˜(í™”ì¬, ê±´ê°•, ìš´ì „, ìë™ì°¨ ë“±)ê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ ì—¬ëŸ¬ í›„ë³´ê°€ ê°€ëŠ¥í•¨' ë“±)"(ê²½ì–´ë¡œ ë‹µë³€í•´ì•¼ í•¨.),
      "candidates": [
        {{
          "input_code": "í›„ë³´ ì…ë ¥ì½”ë“œ 1",         
          "confidence": "í›„ë³´ 1ì˜ ì‹ ë¢°ë„ (ì˜ˆ: 50%)",
          "reason": "ì´ ì½”ë“œê°€ í›„ë³´ì¸ ì´ìœ "(ìŒìŠ´ì²´ë¡œ ë‹µë³€)
        }},
        {{
          "input_code": "í›„ë³´ ì…ë ¥ì½”ë“œ 2",  
          "confidence": "í›„ë³´ 2ì˜ ì‹ ë¢°ë„ (ì˜ˆ: 30%)",
          "reason": "ì´ ì½”ë“œê°€ í›„ë³´ì¸ ì´ìœ "(ìŒìŠ´ì²´ë¡œ ë‹µë³€)
        }}
      ],
      "evidence": "íŒë‹¨ì— ì‚¬ìš©ëœ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©(ì²­í¬) í•˜ë‚˜ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬"
    }}
    ```
    ---
    HUMAN:
    #Question: {question}
    #Context: {context}
    Answer:
""")

# ë‹¨ì¼ í’ˆëª© ì²˜ë¦¬ ì „ìš© ì²´ì¸
classification_chain_single = (
    {"question": itemgetter("question"), "context": itemgetter("context")}
    | prompt_template_single
    | _llm_model
    | StrOutputParser()
)

def fmt_won(x):
    try:
        return f"{int(x):,}ì›"
    except Exception:
        return "0ì›"

def format_extra(t):
    lines = [f"í’ˆëª©ëª…: {t['term']}"]
    if t.get("description"):
        lines.append(f"ì„¤ëª…: {t['description']}")
    if t.get("related_terms"):
        lines.append(f"ê´€ë ¨ì–´: {', '.join(t['related_terms'])}")
    return "\n".join(lines)
        
# ========================================
# Streamlit UI (ì‹¬í”Œí•˜ê²Œ)
# ========================================
# CSS ìŠ¤íƒ€ì¼ - ì¤‘ì•™ ì •ë ¬ê³¼ ì„¸ë ¨ëœ ë””ìì¸
st.markdown(f"""
<style>

/* ë¡œê³  ì¤‘ì•™ ì •ë ¬ */
.categome-logo-container {{
    display: flex;
    justify-content: center;
    align-items: center;
    width: 100%;
    margin-bottom: 20px;
}}

/* ì„¤ëª… ë¬¸êµ¬ ìŠ¤íƒ€ì¼ (ìˆ˜ì •ë¨) */
.categome-caption {{
    text-align: center;
    color: #666;
    margin-bottom: 40px;
    /* í™”ë©´ ë„ˆë¹„ì— ë”°ë¼ í°íŠ¸ í¬ê¸° ìë™ ì¡°ì ˆ (ìµœì†Œ 16px, ìµœëŒ€ 32px) */
    font-size: clamp(16px, 2.5vw, 32px);
    line-height: 1.6;
    /* ìë™ ì¤„ë°”ê¿ˆ ë°©ì§€ */
    white-space: nowrap;
}}

/* ì…ë ¥ í…Œì´ë¸” ìŠ¤íƒ€ì¼ë§ */
.input-table-container {{
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 2px;
    border-radius: 10px;
    margin: 20px 0;
}}

.input-table-inner {{
    background: white;
    border-radius: 8px;
    padding: 20px;
}}

.input-header {{
    font-weight: 600;
    color: #333;
    padding: 10px 0;
    border-bottom: 2px solid #f0f0f0;
    margin-bottom: 10px;
}}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.stButton > button {{
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    padding: 10px 30px;
    font-size: 16px;
    font-weight: 600;
    border-radius: 25px;
    transition: all 0.3s ease;
    white-space: nowrap;
}}

.stButton > button:hover {{
    transform: translateY(-2px);
    box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
}}

/* ì…ë ¥ í•„ë“œ ìŠ¤íƒ€ì¼ */
.stTextInput > div > div > input {{
    border-radius: 8px;
    border: 1px solid #e0e0e0;
    padding: 8px 12px;
}}

.stNumberInput > div > div > input {{
    border-radius: 8px;
    border: 1px solid #e0e0e0;
    padding: 8px 12px;
}}

.input-table-inner:empty {{
    display: none;
    visibility: hidden;
    padding: 0;
    margin: 0;
}}



</style>
""", unsafe_allow_html=True)

# ë¡œê³  ì¤‘ì•™ ì •ë ¬
# st.columns ëŒ€ì‹  CSS flexboxë¥¼ ì´ìš©í•œ ì¤‘ì•™ ì •ë ¬ë¡œ ë³€ê²½í•˜ì—¬ 'wide' ëª¨ë“œì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘
# ë¡œì»¬ ì´ë¯¸ì§€ë¥¼ st.markdownì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Base64ë¡œ ì¸ì½”ë”©
def image_to_base64(path):
    with open(path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

logo_path = "assets/CateGOMe_kor.png"
if os.path.exists(logo_path):
    try:
        logo_base64 = image_to_base64(logo_path)
        st.markdown(f"""
        <div class="categome-logo-container">
            <img src="data:image/png;base64,{logo_base64}" alt="CateGOMe Logo" width="420">
        </div>
        """, unsafe_allow_html=True)
    except Exception as e:
        # íŒŒì¼ì€ ìˆìœ¼ë‚˜ ì½ê¸° ì˜¤ë¥˜ ë“± ë°œìƒ ì‹œ
        st.markdown("<h1 style='text-align: center;'>ğŸ¤– CateGOMe (ë¡œê³  ë¡œë”© ì˜¤ë¥˜)</h1>", unsafe_allow_html=True)
else:
    # ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš°ì˜ ëŒ€ì²´ í…ìŠ¤íŠ¸
    st.markdown("<h1 style='text-align: center;'>ğŸ¤– CateGOMe</h1>", unsafe_allow_html=True)

# ì„¤ëª… ë¬¸êµ¬
st.markdown(f"""
<div class="categome-caption">
ê°€ê³„ë™í–¥ì¡°ì‚¬ í•­ëª©ì½”ë“œ ìë™ë¶„ë¥˜ AIì±—ë´‡, ì¹´í…Œê³ ë¯¸ì…ë‹ˆë‹¤!<br>
ë²ˆê±°ë¡­ê³  ì• ë§¤í•œ ë¶„ë¥˜ì‘ì—…, ì œê°€ ë˜‘ë˜‘í•˜ê²Œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)

# ----------------------------------------------------------
# ì„¸ì…˜ ìŠ¤í† ë¦¬ì§€ ê¸°ë³¸ê°’
# ----------------------------------------------------------
st.session_state.setdefault("results", None)        # ì „ì²´ ê²°ê³¼ ìºì‹œ
st.session_state.setdefault("last_file_name", None) # ì—…ë¡œë“œ íŒŒì¼ ë³€ê²½ ê°ì§€
st.session_state.setdefault("manual_input", [])  # ìˆ˜ë™ ì…ë ¥ ë°ì´í„°
st.session_state.setdefault("uploader_key", 0)      # íŒŒì¼ ì—…ë¡œë” ì´ˆê¸°í™”ìš© ì¹´ìš´í„°
st.session_state.setdefault("input_key_nonce", 0)   # ì§ì ‘ ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”ìš© ì¹´ìš´í„°

# === ì—…ë¡œë” ===
st.markdown("### ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œ")
uploaded_file = st.file_uploader(
    "ê°€ê³„ë¶€ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
    type=['png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp', 'tiff'],
    help="ë“œë˜ê·¸ ì•¤ ë“œë¡­ ë˜ëŠ” í´ë¦­í•˜ì—¬ íŒŒì¼ ì„ íƒ",
    key=f"main_uploader_v3_{st.session_state['uploader_key']}",
)

# íŒŒì¼ ë°”ë€Œë©´ ê²°ê³¼ ì´ˆê¸°í™”
if uploaded_file is not None and st.session_state["last_file_name"] != uploaded_file.name:
    st.session_state["results"] = None
    st.session_state["last_file_name"] = uploaded_file.name

# === ìˆ˜ë™ ì…ë ¥ í…Œì´ë¸” ===
st.markdown("### âœï¸ ì§ì ‘ ì…ë ¥")
st.markdown("í’ˆëª© ì •ë³´ë¥¼ ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ì…ë ¥ í…Œì´ë¸” ì»¨í…Œì´ë„ˆ
with st.container():
    st.markdown('<div class="input-table-container"><div class="input-table-inner">', unsafe_allow_html=True)
    
    # í—¤ë” í–‰
    cols = st.columns([3, 2, 2])
    cols[0].markdown('<div class="input-header">ğŸ“¦ í’ˆëª©ëª…</div>', unsafe_allow_html=True)
    cols[1].markdown('<div class="input-header">ğŸ’° ìˆ˜ì…</div>', unsafe_allow_html=True)
    cols[2].markdown('<div class="input-header">ğŸ’¸ ì§€ì¶œ</div>', unsafe_allow_html=True)
    
# ì…ë ¥ í–‰ë“¤
manual_items = []
for i in range(5):
    cols = st.columns([3, 2, 2])
    with cols[0]:
        name = st.text_input(
            f"í’ˆëª© {i+1}", 
            key=f"name_{st.session_state['input_key_nonce']}_{i}",
            placeholder=f"í’ˆëª© {i+1}",
            label_visibility="collapsed"
        )
    with cols[1]:
        income = st.number_input(
            f"ìˆ˜ì… {i+1}", 
            min_value=0,
            key=f"income_{st.session_state['input_key_nonce']}_{i}",
            label_visibility="collapsed"
            # value=0 ì œê±°
        )
    with cols[2]:
        expense = st.number_input(
            f"ì§€ì¶œ {i+1}", 
            min_value=0,
            key=f"expense_{st.session_state['input_key_nonce']}_{i}",
            label_visibility="collapsed"
            # value=0 ì œê±°
        )
    
    if name:  # í’ˆëª©ëª…ì´ ì…ë ¥ëœ ê²½ìš°ë§Œ ì¶”ê°€
        manual_items.append({"name": name.strip(), "income": income, "expense": expense})
    
    st.markdown('</div></div>', unsafe_allow_html=True)

# ì„¸ì…˜ì— ì €ì¥
st.session_state["manual_items"] = manual_items

# ì…ë ¥ ìƒíƒœ í‘œì‹œ
if manual_items:
    st.success(f"âœ… {len(manual_items)}ê°œ í’ˆëª©ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ----------------------------------------------------------
# ë²„íŠ¼ í™œì„±í™” ì¡°ê±´: ì´ë¯¸ì§€ OR ìˆ˜ë™ì…ë ¥ì´ ìˆìœ¼ë©´ í™œì„±í™”
# ----------------------------------------------------------
can_process = uploaded_file is not None or len(manual_items) > 0

def reset_app_state():
    # 1) ê²°ê³¼/ë©”íƒ€ ìƒíƒœ ì œê±°
    for k in ["results", "manual_items", "last_file_name"]:
        st.session_state.pop(k, None)

    # 2) ì—…ë¡œë” ë¦¬ì…‹: key ì¹´ìš´í„° ì¦ê°€ â†’ ìœ„ì ¯ ì¬ë§ˆìš´íŠ¸ë¡œ íŒŒì¼ ë¹„ìš°ê¸°
    st.session_state["uploader_key"] = st.session_state.get("uploader_key", 0) + 1

    # 3) ì§ì ‘ ì…ë ¥ ë¦¬ì…‹: key ì¹´ìš´í„° ì¦ê°€ â†’ ìœ„ì ¯ ì¬ë§ˆìš´íŠ¸ë¡œ íƒ€ì´í•‘ ê°’ ë¹„ìš°ê¸°
    st.session_state["input_key_nonce"] = st.session_state.get("input_key_nonce", 0) + 1

    # 4) (ì„ íƒ) í”ì  ì²­ì†Œ: ì´ì „ name_/income_/expense_ í‚¤ë“¤ ì œê±°
    #    - ì•ˆ í•´ë„ ë™ì‘ì—” ë¬¸ì œ ì—†ì§€ë§Œ, ì„¸ì…˜ ì˜¤ì—¼ ìµœì†Œí™” ëª©ì 
    for k in list(st.session_state.keys()):
        if re.match(r"^(name|income|expense)_\d+(_\d+)?$", k):
            st.session_state.pop(k, None)
    st.session_state.pop("uploaded_image_v3", None)

    # 5) ì¦‰ì‹œ UI ë°˜ì˜
    # st.rerun()
    
# ----------------------------------------------------------
# ë²„íŠ¼ í™œì„±í™” ì¡°ê±´: ì´ë¯¸ì§€ OR ìˆ˜ë™ì…ë ¥ì´ ìˆìœ¼ë©´ í™œì„±í™”
# ----------------------------------------------------------
can_process = uploaded_file is not None or len(manual_items) > 0

if can_process:
    st.markdown("<br>", unsafe_allow_html=True)
    # ë²„íŠ¼ë“¤ì„ ì¤‘ì•™ì— ë‚˜ë€íˆ ë°°ì¹˜
    _, L_COL, R_COL, _ = st.columns([2, 1, 1, 2])
    with L_COL:
        run = st.button("ğŸš€ ë¶„ë¥˜ ì‹œì‘", type="primary", use_container_width=True, key="run_btn_v3")
    with R_COL:
        # on_clickì— ìœ„ì—ì„œ ì •ì˜í•œ ì½œë°± í•¨ìˆ˜ ì—°ê²° (ì´ì œ í•¨ìˆ˜ê°€ ìœ„ì— ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì •ìƒ ì‘ë™)
        st.button("ì´ˆê¸°í™”", on_click=reset_app_state, key="reset_button_v3", use_container_width=True)

    # ======================================================
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: "if run" ë¸”ë¡ì€ ë²„íŠ¼ ì •ì˜ ë°”ë¡œ ë‹¤ìŒì— ìœ„ì¹˜
    # ======================================================
    if run:
        if classification_chain_single is None:
            st.error("ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        else:
            progress = st.progress(0, "ë¶„ì„ ì¤€ë¹„ ì¤‘...")
            
            # ë‘ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
            all_items = []
            
            # 1. ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œ
            if uploaded_file is not None:
                progress.progress(20, "ğŸ“¸ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
                try:
                    img = Image.open(uploaded_file).convert("RGB")
                    gemini_model = genai.GenerativeModel("gemini-1.5-flash")
                    
                    prompt = """
ê°€ê³„ë¶€ ì‚¬ì§„ì—ì„œ í‘œë¥¼ ì¸ì‹í•´ì„œ ê° í–‰ì˜
1) í’ˆëª©ëª…(= 'ìˆ˜ì…ì¢…ë¥˜ ë° ì§€ì¶œì˜ í’ˆëª…ê³¼ ìš©ë„' ì—´),
2) ìˆ˜ì… ê¸ˆì•¡,
3) ì§€ì¶œ ê¸ˆì•¡
ì„ ì¶”ì¶œí•˜ë¼.

ê·œì¹™:
- ê¸ˆì•¡ì˜ ì‰¼í‘œ(,)ëŠ” ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ.
- ê°’ì´ ë¹„ì–´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ.
- ì œëª©í–‰Â·ì²´í¬ë°•ìŠ¤Â·ë¹ˆì¤„ì€ ì œì™¸.
- ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥.

JSON ìŠ¤í‚¤ë§ˆ:
{
  "items": [
    {"name": "í’ˆëª©ëª…", "income": 0, "expense": 0},
    ...
  ]
}
"""
                    img_bytes = uploaded_file.getvalue()
                    resp = gemini_model.generate_content(
                        [{"text": prompt}, {"inline_data": {"mime_type": uploaded_file.type, "data": img_bytes}}],
                        generation_config={"response_mime_type": "application/json"}
                    )
                    raw = resp.text
                    data = json.loads(raw)
                    
                    # OCR ê²°ê³¼ ì²˜ë¦¬
                    for it in data.get("items", []):
                        name = str(it.get("name", "")).strip()
                        def to_int(x):
                            s = str(x).replace(",", "").strip()
                            return int(re.sub(r"[^\d]", "", s)) if re.search(r"\d", s) else 0
                        income = to_int(it.get("income", 0))
                        expense = to_int(it.get("expense", 0))
                        if name:
                            all_items.append({"name": name, "income": income, "expense": expense})
                except Exception as e:
                    st.warning(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2. ìˆ˜ë™ ì…ë ¥ ë°ì´í„° ì¶”ê°€
            all_items.extend(manual_items)
        
        # í•©ì‚°ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        aggregated_items = {}

        # ëª¨ë“  í’ˆëª©ì„ ìˆœíšŒí•˜ë©° í•©ì‚°
        for item in all_items:
            name = item["name"]
            if name in aggregated_items:
                # ì´ë¯¸ ë“±ë¡ëœ í’ˆëª©ì´ë©´, ìˆ˜ì…ê³¼ ì§€ì¶œì„ ë”í•´ì¤Œ
                aggregated_items[name]["income"] += item["income"]
                aggregated_items[name]["expense"] += item["expense"]
            else:
                # ì²˜ìŒ ë³´ëŠ” í’ˆëª©ì´ë©´, ë”•ì…”ë„ˆë¦¬ì— ìƒˆë¡œ ì¶”ê°€
                aggregated_items[name] = item.copy() # ì›ë³¸ ìˆ˜ì •ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë³µì‚¬

        # ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±
        items = list(aggregated_items.values())
        
        # ì´ì œ items ë¦¬ìŠ¤íŠ¸ë¡œ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì§„í–‰
        product_name_list = [it["name"] for it in items]
        income_list = [it["income"] for it in items]
        expense_list = [it["expense"] for it in items]
        
        progress.progress(30, f"âœ… {len(items)}ê°œ í’ˆëª© ë°œê²¬")

        # ì½”ë“œâ†’í•­ëª©ëª… ë§µ
        # _df['ì…ë ¥ì½”ë“œ_str'] = _df['ì…ë ¥ì½”ë“œ'].astype(str).str.replace(r'\.0$', '', regex=True)
        # code_to_name_map = pd.Series(_df.í•­ëª©ëª….values, index=_df.ì…ë ¥ì½”ë“œ_str).to_dict()
        code_to_name_map = create_extended_code_map(_df)

        # ë²¡í„°ìŠ¤í† ì–´ ë¬¸ì„œ ë©”ëª¨ë¦¬ ë¡œë“œ
        all_docs_from_vs = {name: list(vs.docstore._dict.values()) for name, vs in _vectorstores.items()}

        # ê²°ê³¼ ì»¨í…Œì´ë„ˆ
        definite_results, ambiguous_results, failed_results = [], [], []

        total = max(len(product_name_list), 1)
        for i, pname_orig in enumerate(product_name_list):
            progress.progress(30 + int(60 * (i + 1) / total), f"ğŸ” ë¶„ë¥˜ ì¤‘... ({i+1}/{total}) - {pname_orig}")

            q_single = f"product_name = ['{pname_orig}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
            search_output = search_classification_codes(q_single, all_docs_from_vs, sim_topk_per_term=5, num_related_terms=3)
            pname = (search_output.get("extracted_terms_info") or [{"term": pname_orig}])[0]["term"]

            if "error" in search_output or not search_output["context_docs"]:
                failed_results.append({"í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"})
                continue

            context = "\n\n---\n\n".join([doc.page_content for doc in search_output["context_docs"]])
            context = context.replace("ì¶œì²˜: cases", "ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘").replace("ì¶œì²˜: classification", "ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘")
            extra_info = "\n\n".join(format_extra(t) for t in search_output.get("extracted_terms_info", []))

            if extra_info:
                context = context + "\n\n---\n\n[LLM ë³´ì¡° ì„¤ëª…]\n" + extra_info

            
            final_question = f"product_name = ['{pname}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
            input_data = {"question": final_question, "context": context}

            try:
                out_str = classification_chain_single.invoke(input_data)
                m = re.search(r'\{.*\}', out_str, re.DOTALL)
                llm = json.loads(m.group(0)) if m else {}
                ctype = llm.get("classification_type")

                if ctype == "DEFINITE":
                    r = llm.get("result", {})
                    code = str(r.get("input_code", "")).strip()
                    item_name = code_to_name_map.get(code, "í•­ëª©ëª… ì—†ìŒ")
                    definite_results.append({
                        "í’ˆëª©ëª…": pname, "ì…ë ¥ì½”ë“œ": code, "í•­ëª©ëª…": item_name,
                        "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i],
                        "ì‹ ë¢°ë„": r.get("confidence","N/A"),
                        "ì¶”ë¡  ì´ìœ ": r.get("reason","N/A"), "ê·¼ê±°ì •ë³´": r.get("evidence","N/A")
                    })
                elif ctype == "AMBIGUOUS":
                    cands = llm.get("candidates", [])
                    for c in cands:
                        candidate_code = str(c.get("input_code","")).strip()
                        item_name = code_to_name_map.get(candidate_code, "í•­ëª©ëª… ì—†ìŒ")
                        c["í•­ëª©ëª…"] = item_name
                        # c["í•­ëª©ëª…"] = code_to_name_map.get(str(c.get("input_code","")).strip(), "í•­ëª©ëª… ì—†ìŒ")
                    ambiguous_results.append({
                        "í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i],
                        "ëª¨í˜¸ì„± ì´ìœ ": llm.get("reason_for_ambiguity","N/A"),
                        "í›„ë³´": cands, "ê·¼ê±°ì •ë³´": llm.get("evidence","N/A")
                    })
                else:
                    failed_results.append({"í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…: {ctype}"})
            except Exception as e:
                failed_results.append({"í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": str(e)})

        # ----- DataFrame ìƒì„± ë° ìˆ«ìí˜•ìœ¼ë¡œ ê°•ì œ(âš ï¸ì œê±° í•µì‹¬) -----
        df_definite = pd.DataFrame(definite_results)
        if not df_definite.empty:
            for col in ["ìˆ˜ì…", "ì§€ì¶œ"]:
                df_definite[col] = pd.to_numeric(df_definite[col], errors="coerce").fillna(0).astype(int)

        # ìºì‹œì— ì €ì¥ (ë‹¤ìŒ rerunì—ì„œ ì¬ì‚¬ìš©)
        st.session_state["results"] = {
            "df_definite": df_definite,
            "ambiguous_results": ambiguous_results,
            "failed_results": failed_results,
        }
        
        progress.progress(100, "âœ… ë¶„ë¥˜ ì™„ë£Œ!")

# ======================================================
# 2) ë Œë”ë§: resultsê°€ ìˆìœ¼ë©´ ì¬ê³„ì‚° ì—†ì´ ê·¸ëŒ€ë¡œ í‘œì‹œ
#    (ì²´í¬ë°•ìŠ¤ ëˆŒëŸ¬ë„ â€˜ë‹¤ì‹œ ë¶„ë¥˜â€™ ì•ˆ ëŒì•„ê°)
# ======================================================
results = st.session_state.get("results")
if results is not None:
    df_definite        = results["df_definite"]
    ambiguous_results  = results["ambiguous_results"]
    failed_results     = results["failed_results"]
    
    st.markdown("---")
    st.markdown("## ğŸ“Š ë¶„ë¥˜ ê²°ê³¼")

    # --- (1) ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª© ---
    if not df_definite.empty:
        st.markdown("### âœ… ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©")
        view_def = df_definite.copy()
        view_def["ìˆ˜ì…(ì›)"] = view_def["ìˆ˜ì…"].apply(fmt_won)
        view_def["ì§€ì¶œ(ì›)"] = view_def["ì§€ì¶œ"].apply(fmt_won)
        view_def = view_def[["í’ˆëª©ëª…", "ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ìˆ˜ì…(ì›)", "ì§€ì¶œ(ì›)"]]
        sty = (
            view_def
            .style
            .set_properties(subset=["ìˆ˜ì…(ì›)", "ì§€ì¶œ(ì›)"], **{"text-align": "right"})
        )
        # st.tableì€ Stylerë¥¼ ë°˜ì˜í•´ ì •ë ¬ì´ ë¨¹ìŒ
        st.table(sty)
    
    # --- (2) ì…ë ¥ì½”ë“œë³„ ìš”ì•½ ë³´ê¸° (ì¬ê³„ì‚° ì—†ì´ ìºì‹œë¡œë¶€í„°) ---
    if st.checkbox("ì…ë ¥ì½”ë“œë³„ ìš”ì•½ ë³´ê¸°", key="show_summary"):
        if not df_definite.empty:
            numeric_codes_mask = pd.to_numeric(df_definite['ì…ë ¥ì½”ë“œ'], errors='coerce').notna()
            df_summary = df_definite[numeric_codes_mask].copy()
            
            if not df_summary.empty:
                df_summary['ì…ë ¥ì½”ë“œ'] = df_summary['ì…ë ¥ì½”ë“œ'].astype(float).astype(int)
                df_summary_agg = df_summary.groupby('ì…ë ¥ì½”ë“œ').agg(
                    í•­ëª©ëª…=('í•­ëª©ëª…', 'first'),
                    ìˆ˜ì…í•©ê³„=('ìˆ˜ì…', 'sum'),
                    ì§€ì¶œí•©ê³„=('ì§€ì¶œ', 'sum'),
                    í•´ë‹¹í’ˆëª©ëª…=('í’ˆëª©ëª…', lambda x: ', '.join(x))
                ).reset_index()
                
                view_sum = df_summary_agg.copy()
                view_sum["ìˆ˜ì…í•©ê³„(ì›)"] = view_sum["ìˆ˜ì…í•©ê³„"].apply(fmt_won)
                view_sum["ì§€ì¶œí•©ê³„(ì›)"] = view_sum["ì§€ì¶œí•©ê³„"].apply(fmt_won)
                view_sum = view_sum[['ì…ë ¥ì½”ë“œ', 'í•­ëª©ëª…', 'ìˆ˜ì…í•©ê³„(ì›)', 'ì§€ì¶œí•©ê³„(ì›)', 'í•´ë‹¹í’ˆëª©ëª…']]
                
                sty2 = (
                    view_sum
                    .style
                    .set_properties(subset=["ìˆ˜ì…í•©ê³„(ì›)", "ì§€ì¶œí•©ê³„(ì›)"], **{"text-align": "right"})
                )
                # st.tableì€ Stylerë¥¼ ë°˜ì˜í•´ ì •ë ¬ì´ ë¨¹ìŒ
                st.table(sty2)
            else:
                st.warning("ìˆ«ì ì½”ë“œê°€ ìˆëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # --- (3) ëª…í™•í•œ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±° ---
    if not df_definite.empty:
        with st.expander("ğŸ” ëª…í™•í•œ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±°", expanded=False):
            for row in df_definite.to_dict(orient="records"):
                st.markdown(
                    f"**í’ˆëª©ëª…: {row['í’ˆëª©ëª…']} (ì„ íƒëœ ì½”ë“œ: {row['ì…ë ¥ì½”ë“œ']}, "
                    f"í•­ëª©ëª…: {row['í•­ëª©ëª…']}, ì‹ ë¢°ë„: {row['ì‹ ë¢°ë„']})**"
                )
                if row.get("ì¶”ë¡  ì´ìœ "):
                    st.write(f"**- ì¶”ë¡  ì´ìœ :** {row['ì¶”ë¡  ì´ìœ ']}")
                if row.get("ê·¼ê±°ì •ë³´"):
                    st.write("**- í•µì‹¬ ê·¼ê±°:**")
                    st.code(row["ê·¼ê±°ì •ë³´"])
                st.markdown("---")
    
    # --- (4) ì‚¬ìš©ì ê²€í† ê°€ í•„ìš”í•œ í’ˆëª© (ì»¬ëŸ¼ëª… í•œê¸€í™”) ---
    if ambiguous_results:
        st.markdown("### âš ï¸ ì‚¬ìš©ì ê²€í† ê°€ í•„ìš”í•œ í’ˆëª©")
        st.info("ì•„ë˜ í’ˆëª©ë“¤ì€ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹¨ì¼ ì½”ë“œë¥¼ í™•ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        for result in ambiguous_results:
            with st.expander(f"ğŸ“Œ {result['í’ˆëª©ëª…']} (ìˆ˜ì…: {fmt_won(result['ìˆ˜ì…'])}, ì§€ì¶œ: {fmt_won(result['ì§€ì¶œ'])})"):
                st.write(f"**ê²€í†  í•„ìš” ì´ìœ :** {result['ëª¨í˜¸ì„± ì´ìœ ']}")
                candidates_df = pd.DataFrame(result['í›„ë³´']).rename(columns={
                    "input_code": "ì…ë ¥ì½”ë“œ",
                    "confidence": "ì‹ ë¢°ë„",
                    "reason": "ê·¼ê±°ì •ë³´",
                })
                # í‘œì‹œ ì»¬ëŸ¼ ìˆœì„œ ê³ ì •
                view_cols = [c for c in ["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ê·¼ê±°ì •ë³´"] if c in candidates_df.columns]
                h3 = min(44 * (len(candidates_df) + 1), 400)
                st.dataframe(candidates_df[view_cols], use_container_width=True, height=h3, hide_index=True)
    
    # --- (5) ì‹¤íŒ¨ í•­ëª© ---
    if failed_results:
        with st.expander("âŒ ì²˜ë¦¬ ì‹¤íŒ¨ í•­ëª©"):
            df_failed = pd.DataFrame(failed_results)
            h4 = min(44 * (len(df_failed) + 1), 400)
            st.dataframe(df_failed, use_container_width=True, height=h4, hide_index=True)

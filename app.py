# ========================================
# ğŸ”§ ì„¤ì •ê°’ (Colab ì½”ë“œ ê·¸ëŒ€ë¡œ)
# ========================================
import streamlit as st

# API Key ì„¤ì • (Streamlit secrets ì‚¬ìš©)
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GENAI_API_KEY = st.secrets["GENAI_API_KEY"]

# --- Global (1íšŒ ë¡œë“œ ìºì‹œ) ----------------------------------------------------
EMBED_MODEL = "text-embedding-3-large"
LLM_MODEL = "gpt-4o"  # í†µí•© ëª¨ë¸ëª… ë³€ìˆ˜ ì‚¬ìš©

VECTORSTORE_DIR_CASES = "vectorstores/cases"
INDEX_NAME_CASES = "cases_index"
VECTORSTORE_DIR_CLASSIFICATION = "vectorstores/classification"
INDEX_NAME_CLASSIFICATION = "classification_index"
CSV_PATH = "data/classification_code.csv"

REQUIRED_COLS = ["í•­ëª©ëª…", "ì…ë ¥ì½”ë“œ", "ì²˜ë¦¬ì½”ë“œ", "í•­ëª©ë¶„ë¥˜ë‚´ìš©", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"]

# ========================================
# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (Colab ì½”ë“œ ê·¸ëŒ€ë¡œ)
# ========================================
import os
import re
import json
import ast
from typing import List, Dict, Any
from operator import itemgetter

import pandas as pd
from PIL import Image
import google.generativeai as genai

from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# Gemini ì„¤ì •
genai.configure(api_key=GENAI_API_KEY)

# ========================================
# Streamlit í˜ì´ì§€ ì„¤ì •
# ========================================
st.set_page_config(
    page_title="ì¹´í…Œê³ ë¯¸ - í†µê³„ì²­ í•­ëª©ìë™ë¶„ë¥˜AI",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ========================================
# Colab ì´ˆê¸°í™” ì½”ë“œ ê·¸ëŒ€ë¡œ (ìºì‹± ì¶”ê°€)
# ========================================
@st.cache_resource
def initialize_system():
    try:
        _embeddings = OpenAIEmbeddings(model=EMBED_MODEL, openai_api_key=OPENAI_API_KEY)

        # === ìˆ˜ì •ëœ ë¶€ë¶„: ë‘ ê°œì˜ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ===
        _vectorstore_cases = FAISS.load_local(
            folder_path=VECTORSTORE_DIR_CASES,
            embeddings=_embeddings,
            index_name=INDEX_NAME_CASES,
            allow_dangerous_deserialization=True
        )

        _vectorstore_classification = FAISS.load_local(
            folder_path=VECTORSTORE_DIR_CLASSIFICATION,
            embeddings=_embeddings,
            index_name=INDEX_NAME_CLASSIFICATION,
            allow_dangerous_deserialization=True
        )

        # ë²¡í„°ìŠ¤í† ì–´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬í•˜ì—¬ í™•ì¥ì„± í™•ë³´
        _vectorstores = {
            "cases": _vectorstore_cases,
            "classification": _vectorstore_classification
        }
        # ============================================

        _df = pd.read_csv(CSV_PATH, encoding='utf-8')
        missing = [c for c in REQUIRED_COLS if c not in _df.columns]
        if missing:
            raise KeyError(f"ERROR[csv]: Missing required columns: {missing}")
        # ê³ ìœ í‚¤(ì¤‘ë³µ ì œê±°ìš©) ì—†ìœ¼ë©´ í–‰ ì¸ë±ìŠ¤ ì‚¬ìš©
        _df = _df.reset_index(drop=False).rename(columns={"index": "_rowid"})

        # LLM ëª¨ë¸ë„ ìºì‹œ
        _llm_model = ChatOpenAI(
            model_name=LLM_MODEL,
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY
        )

        return _embeddings, _vectorstores, _df, _llm_model

    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None, None, None

# ì´ˆê¸°í™”
_embeddings, _vectorstores, _df, _llm_model = initialize_system()

# ========================================
# Colab í—¬í¼ í•¨ìˆ˜ë“¤ ê·¸ëŒ€ë¡œ
# ========================================

        
def _short_doc_from_row(row: pd.Series) -> Document:
    """
    page_contentëŠ” í† í° ë‚­ë¹„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•µì‹¬ í•„ë“œë§Œ.
    ë‚˜ë¨¸ì§€ëŠ” metadataì— ë‹´ëŠ”ë‹¤.
    'ì¶œì²˜' ì •ë³´ë¥¼ page_content ë§¨ ì•ì— ì¶”ê°€í•˜ê³ , 'ì…ë ¥ì½”ë“œ'ë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    source = row.get('ì¶œì²˜', 'í•­ëª©ë¶„ë¥˜ì§‘')
    source_info = f"ì¶œì²˜: {source}\n"

    core_fields_order = [col for col in ["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "í•­ëª©ë¶„ë¥˜ë‚´ìš©", "ì²˜ë¦¬ì½”ë“œ", "í¬í•¨í•­ëª©", "ì œì™¸í•­ëª©"] if col in row.index]

    core_lines = []
    for col in core_fields_order:
        value = row[col]

        # 'ì…ë ¥ì½”ë“œ' ì»¬ëŸ¼ì¼ ê²½ìš°, ì •ìˆ˜ë¡œ ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤.
        if col == "ì…ë ¥ì½”ë“œ":
            try:
                # floatìœ¼ë¡œ ë¨¼ì € ë³€í™˜ í›„ intë¡œ ë³€í™˜í•˜ì—¬ "720.0" ê°™ì€ ë¬¸ìì—´ë„ ì²˜ë¦¬
                value_str = str(int(float(value)))
            except (ValueError, TypeError):
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ (ì˜ˆ: NaN, ë¹„ìˆ«ì ë¬¸ìì—´) ì›ë³¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                value_str = str(value)
        else:
            value_str = str(value)
        # ============================

        core_lines.append(f"{col}: {value_str}")

    page = source_info + "\n".join(core_lines)
    meta = row.to_dict()
    return Document(page_content=page, metadata=meta)


def _keyword_search(df: pd.DataFrame, term: str) -> List[Document]:
    """ë¶€ë¶„ì¼ì¹˜ contains, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ. ìƒí•œ ì—†ìŒ(ìš”ì²­ ë°˜ì˜)."""
    if df is None:  # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        return []
    # NaN ì•ˆì „ ì²˜ë¦¬ ë° íƒ€ì… ë³€í™˜
    df_copy = df.copy()  # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³€ê²½ ë°©ì§€
    for c in REQUIRED_COLS:
        if c in df_copy.columns and df_copy[c].dtype != object:
            df_copy[c] = df_copy[c].astype(str)

    mask = (
        df_copy["í•­ëª©ë¶„ë¥˜ë‚´ìš©"].str.contains(term, case=False, na=False) |
        df_copy["í•­ëª©ëª…"].str.contains(term, case=False, na=False) |
        df_copy["í¬í•¨í•­ëª©"].str.contains(term, case=False, na=False) |
        df_copy["ì œì™¸í•­ëª©"].str.contains(term, case=False, na=False)
    )
    sub = df_copy.loc[mask]
    # ì¤‘ë³µ ì œê±°(í–‰ ì¸ë±ìŠ¤ ê¸°ë°˜)
    sub = sub.drop_duplicates(subset=["_rowid"], keep="first")
    return [_short_doc_from_row(r) for _, r in sub.iterrows()]


def _keyword_search_on_docs(docs: List[Document], term: str) -> List[Document]:
    """ë©”ëª¨ë¦¬ì— ë¡œë“œëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if not docs:
        return []

    # page_contentì— termì´ í¬í•¨ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    return [doc for doc in docs if term.lower() in doc.page_content.lower()]


def _similarity_topk_for_term(vs: FAISS, embeddings: OpenAIEmbeddings, term: str, k: int = 2) -> List[Document]:
    if vs is None or embeddings is None:  # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        return []
    retriever = vs.as_retriever(
        search_type="mmr",  # MMR ì‚¬ìš© ìœ ì§€
        search_kwargs={"k": k, "fetch_k": 30, "lambda_mult": 0.5}
    )
    return retriever.invoke(term)

def _get_term_info_via_llm(llm: ChatOpenAI, user_query: str, num_related_terms: int = 3) -> List[Dict[str, Any]]:
    """
    LLMì„ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í’ˆëª©ëª…ë“¤ì„ ì¶”ì¶œí•˜ê³ , ê° í’ˆëª©ëª…ì— ëŒ€í•œ ì„¤ëª…ê³¼ ê´€ë ¨ ìš©ì–´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
    ì•ˆì •ì ì¸ JSON ì¶”ì¶œì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ì™€ íŒŒì‹± ë¡œì§ì´ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤.
    """
    if llm is None:
        return []

    # === í’ˆëª© ì„¤ëª… ë° ê´€ë ¨ì–´ ë°˜í™˜ í”„ë¡¬í”„íŠ¸ (Colab ê·¸ëŒ€ë¡œ) ===
    prompt = f"""
ë„ˆëŠ” ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ 'product_name' ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  í’ˆëª©ëª…ì„ ë¶„ì„í•˜ê³ , ì˜¤íƒˆìë¥¼ êµì •í•œ ë’¤ ê²€ìƒ‰ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ AIì´ë‹¤.

## ì‘ì—… ì ˆì°¨ (ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ë”°ë¥¼ ê²ƒ) ##
1. **í’ˆëª©ëª… ì¶”ì¶œ:** `product_name = [...]` ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ëª¨ë“  ì›ë³¸ í’ˆëª©ëª…ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œí•œë‹¤.
2. **ì˜¤íƒˆì êµì •:** ê° ì›ë³¸ í’ˆëª©ëª…ì˜ ì˜¤íƒˆìë‚˜ ë¶ˆë¶„ëª…í•œ í‘œí˜„ì„ ê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  ì¼ë°˜ì ì¸ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•œë‹¤. (ì˜ˆ: "íŒŒí”Œë ˆì‹œí‹°" -> "í¼í”Œë ‰ì‹œí‹°") ë§Œì•½ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ë‹¤ë©´ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.
3. **ì„¤ëª… ìƒì„±:** **ìˆ˜ì •ëœ í’ˆëª©ëª…**ì— ëŒ€í•´, ê·¸ í’ˆëª©ì˜ ë³¸ì§ˆê³¼ ëª©ì ì„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•œë‹¤.
4. **ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ (ë§¤ìš° ì¤‘ìš”):**
   - **ìˆ˜ì •ëœ í’ˆëª©ëª…**ê³¼ **ë„¤ê°€ ì‘ì„±í•œ ì„¤ëª…**ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬, ê²€ìƒ‰ì— ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” í•µì‹¬ ê´€ë ¨ ìš©ì–´ë¥¼ {num_related_terms}ê°œ ì¶”ì¶œí•œë‹¤.
   - **ë‹¨ìˆœ ë™ì˜ì–´ë¥¼ ë„˜ì–´, ê·¸ í’ˆëª©ì˜ 'ëª©ì 'ì´ë‚˜ 'ìƒìœ„ ì¹´í…Œê³ ë¦¬'ì— í•´ë‹¹í•˜ëŠ” ê°œë…ì ì¸ ë‹¨ì–´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•œë‹¤.** (ì˜ˆ: 'ì „ê¸°ì°¨ ì¶©ì „'ì˜ ëª©ì ì€ 'ì—°ë£Œ'ë¥¼ ì±„ìš°ëŠ” ê²ƒì´ë¯€ë¡œ 'ì—°ë£Œ'ë‚˜ 'ì—ë„ˆì§€'ë¥¼ í¬í•¨)

5. **JSON ì¶œë ¥:** ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì•„ë˜ "ì¶œë ¥ ì˜ˆì‹œ"ì™€ **ì™„ë²½í•˜ê²Œ ë™ì¼í•œ JSON í˜•ì‹**ìœ¼ë¡œë§Œ ì‘ë‹µí•œë‹¤.

## ì…ë ¥ ë° ì¶œë ¥ ì˜ˆì‹œ (ë§¤ìš° ì¤‘ìš”) ##
### ì…ë ¥ ì¿¼ë¦¬ ì˜ˆì‹œ:
'''product_name = ['ëª¨ë‘ ìŒˆ', 'ì „ê¸°ì°¨ ì¶©ì „', 'íŒŒí”Œë ˆì‹œí‹°'] ...'''

### ë„ˆì˜ JSON ì¶œë ¥ ì˜ˆì‹œ:
```json
{{
  "terms": [
    {{
      "original_term": "ëª¨ë‘ ìŒˆ",
      "term": "ëª¨ë‘ ìŒˆ",
      "description": ìƒì¶”Â·ê¹»ì ë“± ë‹¤ì–‘í•œ ìŒˆì±„ì†Œë¥¼ í•œë° ë¬¶ì–´ ì œê³µí•˜ëŠ” êµ¬ì„±ìœ¼ë¡œ, ë°¥ê³¼ ê³ ê¸°, ìŒˆì¥ì„ ê³ë“¤ì—¬ ì‹¸ ë¨¹ëŠ” í•œêµ­ì‹ ì‹ì‚¬ ë°©ì‹ì…ë‹ˆë‹¤. ì±„ì†Œ ì„­ì·¨ë¥¼ ëŠ˜ë¦¬ê³  ë‹´ë°±í•˜ê²Œ ì¦ê¸°ë ¤ëŠ” ëª©ì ì˜ ë©”ë‰´ë¡œë„ í™œìš©ë©ë‹ˆë‹¤.",
      "related_terms": [í•œì‹", "ìŒˆ", "ì±„ì†Œ"]
    }},
    {{
      "original_term": "ì „ê¸°ì°¨ ì¶©ì „",
      "term": "ì „ê¸°ì°¨ ì¶©ì „",
      "description": "ì „ê¸° ìë™ì°¨ì˜ ë°°í„°ë¦¬ì— ì „ë ¥ì„ ê³µê¸‰í•˜ëŠ” í–‰ìœ„ì…ë‹ˆë‹¤. ìë™ì°¨ë¥¼ ìš´í–‰í•˜ê¸° ìœ„í•œ ì—ë„ˆì§€ë¥¼ ì±„ìš°ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
      "related_terms": ["ì „ê¸°ì°¨", "ì¶©ì „ì†Œ", "ì—°ë£Œ"]
    }},
    {{
      "original_term": "íŒŒí”Œë ˆì‹œí‹°",
      "term": "í¼í”Œë ‰ì‹œí‹°",
      "description": "ì‹¤ì‹œê°„ ì›¹ê²€ìƒ‰ê³¼ ìƒì„±í˜• AI ì§ˆì˜ì‘ë‹µì„ ê²°í•©í•œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ìš”ì•½ëœ ë‹µë³€ê³¼ í•¨ê»˜ ê´€ë ¨ ì¶œì²˜ë¥¼ ì œì‹œí•´ ì‹ ì†í•œ ì •ë³´íƒìƒ‰Â·ë¦¬ì„œì¹˜ë¥¼ ë•ìŠµë‹ˆë‹¤.",
      "related_terms": ["AI ê²€ìƒ‰ì—”ì§„", "ì›¹ê²€ìƒ‰", "AI"]
    }}
  ]
}}
---
ì´ì œ ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•´ë¼. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ê³  JSONë§Œ ì¶œë ¥í•´ë¼.

ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬: {user_query}
"""

    try:
        res = llm.invoke(prompt)
        text_content = res.content.strip()

        json_match = re.search(r'\{.*\}', text_content, re.DOTALL)
        if not json_match:
            raise json.JSONDecodeError("LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", text_content, 0)

        json_str = json_match.group(0)
        data = json.loads(json_str)

        terms_info = data.get("terms", [])

        # ê¸°ë³¸ ì •ë¦¬
        cleaned_terms_info = []
        for item in terms_info:
            if item.get("term") and item.get("description"):
                cleaned_related = []
                for rt in item.get("related_terms", []):
                    rt_norm = re.sub(r"\s+", " ", str(rt)).strip().lower()
                    if rt_norm:
                        cleaned_related.append(re.sub(r"\s+", " ", str(rt)).strip())
                cleaned_terms_info.append({
                    "term": re.sub(r"\s+", " ", str(item["term"])).strip(),
                    "description": item["description"].strip(),
                    "related_terms": cleaned_related[:num_related_terms]
                })
        return cleaned_terms_info

    except Exception as e:
        # í´ë°± ë¡œì§
        return [{"term": user_query, "description": "", "related_terms": []}]

# search_classification_codes í•¨ìˆ˜ (Colab ê·¸ëŒ€ë¡œ)
def search_classification_codes(
    user_query: str,
    all_docs_from_vs: Dict[str, List[Document]],  # íŒŒë¼ë¯¸í„°
    sim_topk_per_term: int = 3,  # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
    num_related_terms: int = 3  # LLM ê´€ë ¨ ìš©ì–´ ê°œìˆ˜
) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•´ ë¶„ë¥˜ ì½”ë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    (Colab ì½”ë“œ ê·¸ëŒ€ë¡œ)
    """
    # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
    if _df is None or _embeddings is None or _vectorstores is None or _llm_model is None or OPENAI_API_KEY is None:
        return {
            "query": user_query,
            "extracted_terms_info": [],
            "results": {"keyword": [], "similarity": []},
            "context_docs": [],
            "error": "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (ë°ì´í„°, ë²¡í„°ìŠ¤í† ì–´, LLM ë˜ëŠ” API í‚¤). ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        }

    if not isinstance(user_query, str) or not user_query.strip():
        return {
            "query": user_query,
            "extracted_terms_info": [],
            "results": {"keyword": [], "similarity": []},
            "context_docs": [],
            "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤."
        }

    # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª…, ê´€ë ¨ ìš©ì–´ ë°›ê¸°
    extracted_terms_info = _get_term_info_via_llm(_llm_model, user_query, num_related_terms=num_related_terms)

    all_relevant_docs: List[Document] = []  # í‚¤ì›Œë“œ ë˜ëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    seen_docs_page_content = set()  # ì¤‘ë³µ ì œê±°ìš©

    all_keyword_docs_raw: List[Document] = []  # ë””ë²„ê¹…ìš©: ì¤‘ë³µ í¬í•¨ í‚¤ì›Œë“œ ê²°ê³¼
    all_similarity_docs_raw: List[Document] = []  # ë””ë²„ê¹…ìš©: ì¤‘ë³µ í¬í•¨ ìœ ì‚¬ë„ ê²°ê³¼

    for item in extracted_terms_info:
        term = item["term"]
        description = item["description"]
        related_terms = item.get("related_terms", [])

        # 2. ê° í•µì‹¬ ìš©ì–´(ì›ì–´)ì™€ ê´€ë ¨ ìš©ì–´ì— ëŒ€í•´ í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰
        terms_to_keyword_search = [term] + related_terms
        for search_term in terms_to_keyword_search:
            kw_docs = _keyword_search(_df, search_term)
            if kw_docs:
                all_keyword_docs_raw.extend(kw_docs)  # ë””ë²„ê¹…ìš© ê²°ê³¼ ì¶”ê°€
                for doc in kw_docs:
                    if doc.page_content not in seen_docs_page_content:
                        all_relevant_docs.append(doc)
                        seen_docs_page_content.add(doc.page_content)

        # === ê²€ìƒ‰ B: ë²¡í„°ìŠ¤í† ì–´ì— ëŒ€í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ ===
        for name, doc_list in all_docs_from_vs.items():
            for search_term in terms_to_keyword_search:
                kw_docs_vs = _keyword_search_on_docs(doc_list, search_term)
                if kw_docs_vs:
                    docs_to_add = [Document(page_content=f"ì¶œì²˜: {name}\n{doc.page_content}", metadata=doc.metadata) for doc in kw_docs_vs]
                    all_keyword_docs_raw.extend(docs_to_add)
                    for doc in docs_to_add:
                        if doc.page_content not in seen_docs_page_content:
                            all_relevant_docs.append(doc)
                            seen_docs_page_content.add(doc.page_content)

        # === ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰ ===
        if description:
            for name, vs in _vectorstores.items():
                sim_docs = _similarity_topk_for_term(vs, _embeddings, description, k=sim_topk_per_term)
                if sim_docs:
                    docs_to_add = []
                    for doc in sim_docs:
                        new_doc = Document(page_content=f"ì¶œì²˜: {name}\n{doc.page_content}", metadata=doc.metadata)
                        docs_to_add.append(new_doc)

                    all_similarity_docs_raw.extend(docs_to_add)
                    for doc in docs_to_add:
                        if doc.page_content not in seen_docs_page_content:
                            all_relevant_docs.append(doc)
                            seen_docs_page_content.add(doc.page_content)

    # 4. ìˆ˜ì§‘ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±° (all_relevant_docsì— ì´ë¯¸ ì¤‘ë³µ ì œê±°ë˜ì–´ ìˆ˜ì§‘ë¨)
    unique_docs_objects = all_relevant_docs  # ë³€ìˆ˜ëª… í†µì¼

    return {
        "query": user_query,
        "extracted_terms_info": extracted_terms_info,
        "results": {
            "keyword": all_keyword_docs_raw,  # ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (ì¤‘ë³µ í¬í•¨ ê°€ëŠ¥)
            "similarity": all_similarity_docs_raw  # ëª¨ë“  ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ (ì¤‘ë³µ í¬í•¨ ê°€ëŠ¥)
        },
        "context_docs": unique_docs_objects  # GPTì— ì „ë‹¬í•  ìµœì¢… ì¤‘ë³µ ì œê±°ëœ Document ê°ì²´ ëª©ë¡
    }

# prompt_template_single (Colab í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
prompt_template_single = PromptTemplate.from_template("""
    SYSTEM: ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ 'ì…ë ¥ì½”ë“œ'ì™€ 'í•­ëª©ëª…'ì„ ì¶”ë¡ í•˜ëŠ”, ê·¹ë„ë¡œ ê¼¼ê¼¼í•˜ê³  ê·œì¹™ì„ ì—„ìˆ˜í•˜ëŠ” ë°ì´í„° ë¶„ë¥˜ AIì´ë©°, ë‹¹ì‹ ì˜ ì´ë¦„ì€ "ì¹´í…Œê³ ë¯¸(CateGOMe)"ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

    ## ì ˆëŒ€ ê·œì¹™ (ê°€ì¥ ì¤‘ìš”! ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ) ##
    1. **ìˆ˜ì…/ì§€ì¶œ ê·œì¹™:** `question`ì˜ `expense` ê°’ì´ 0ë³´ë‹¤ í¬ë©´, `input_code`ëŠ” **ì ˆëŒ€ë¡œ 1000 ë¯¸ë§Œì´ ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** ë°˜ëŒ€ë¡œ `income` ê°’ì´ 0ë³´ë‹¤ í¬ë©´, `input_code`ëŠ” **ì ˆëŒ€ë¡œ 1000 ì´ìƒì´ ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** ì˜ˆì™¸ëŠ” ì—†ìŠµë‹ˆë‹¤.
    2. **ì •ë³´ ìš°ì„ ìˆœìœ„ ê·œì¹™:** `context`ì—ì„œ `ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘`(ë˜ëŠ” cases) ì •ë³´ëŠ” `ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘` ì •ë³´ë³´ë‹¤ **í•­ìƒ ìš°ì„ **í•©ë‹ˆë‹¤. ë§Œì•½ ë‘ ì •ë³´ê°€ ì¶©ëŒí•˜ë©´, ë¬´ì¡°ê±´ 'ì¡°ì‚¬ì‚¬ë¡€ì§‘'ì˜ ì½”ë“œë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.

    ## ì‘ì—… ì ˆì°¨ ##
    1. **ì…ë ¥ ë¶„ì„:** `question`ì˜ `í’ˆëª©ëª…`, `income`, `expense` ê°’ì„ í™•ì¸í•˜ê³  [ì ˆëŒ€ ê·œì¹™ 1]ì„ ê¸°ì–µí•©ë‹ˆë‹¤.
    2. **ì»¨í…ìŠ¤íŠ¸ ë¶„ì„:**
        - `í’ˆëª©ëª…`ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” **'ì¡°ì‚¬ì‚¬ë¡€ì§‘'** ë‚´ìš©ì´ ìˆëŠ”ì§€ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
        - ë§Œì•½ ëª…í™•í•œ ì‚¬ë¡€ê°€ ìˆë‹¤ë©´, [ì ˆëŒ€ ê·œì¹™ 2]ì— ë”°ë¼ í•´ë‹¹ ì½”ë“œë¥¼ **ìµœìš°ì„  í›„ë³´**ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.
        - ëª…í™•í•œ ì‚¬ë¡€ê°€ ì—†ë‹¤ë©´, 'í•­ëª©ë¶„ë¥˜ì§‘'ì—ì„œ ê°€ì¥ ì í•©í•œ ì •ì˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    3. **ë¶„ë¥˜ íƒ€ì… ê²°ì •:**
        - **(DEFINITE ì¡°ê±´):** ìœ„ì˜ ê³¼ì •ì„ ê±°ì³, ë‹¨ í•˜ë‚˜ì˜ ì…ë ¥ì½”ë“œë¥¼ 90% ì´ìƒì˜ ì‹ ë¢°ë„ë¡œ í™•ì‹ í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì—ë§Œ "DEFINITE"ë¡œ ê²°ì •í•©ë‹ˆë‹¤. (ì˜ˆ: 'ì±—ì§€í”¼í‹°' -> 'ì±—ì§€í”¼í‹° êµ¬ë…ë£Œ' ì‚¬ë¡€ê°€ ëª…í™•íˆ ì¡´ì¬)
        - **(AMBIGUOUS ì¡°ê±´):** ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹í•˜ë©´ **ë°˜ë“œì‹œ "AMBIGUOUS"**ë¡œ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            - í’ˆëª©ëª…ì´ ë„ˆë¬´ ì¼ë°˜ì ì´ì–´ì„œ ì—¬ëŸ¬ ì½”ë“œê°€ í›„ë³´ê°€ ë  ë•Œ (ì˜ˆ: 'ê³ ë“±ì–´' -> ê°„ê³ ë“±ì–´? ë°”ë‹¤ì–´ë¥˜? ìˆ˜ì‚°ë™ë¬¼í†µì¡°ë¦¼? ì•Œ ìˆ˜ ì—†ìŒ)
            - **í’ˆëª©ëª…ì´ íŠ¹ì • íšŒì‚¬ ì´ë¦„ì´ê³ , ê·¸ íšŒì‚¬ê°€ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ìƒí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš° (ì˜ˆ: 'ë„¤ì´ë²„'  -> ì˜¨ë¼ì¸ì‡¼í•‘ëª°, í˜ì´ ê²°ì œ, ì›¹íˆ° ë“± ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ ìƒí’ˆì´ ìˆì–´ í•˜ë‚˜ë¡œ íŠ¹ì • ë¶ˆê°€)**
            - ì†Œë“ì˜ ì£¼ì²´(ê°€êµ¬ì£¼, ë°°ìš°ì, ê¸°íƒ€ê°€êµ¬ì› ë“±)ê°€ ë¶ˆëª…í™•í•˜ì—¬ ì—¬ëŸ¬ ì½”ë“œê°€ í›„ë³´ê°€ ë  ë•Œ (ì˜ˆ: 'ê¸‰ì—¬' -> ê°€êµ¬ì£¼ê¸‰ì—¬? ë°°ìš°ìê¸‰ì—¬? ê¸°íƒ€ê°€êµ¬ì›ê¸‰ì—¬?)
    4. **JSON ì¶œë ¥:**ê²°ì •ëœ ë¶„ë¥˜ íƒ€ì…ì— ë§ëŠ” JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

    ---
    ## ì¢‹ì€ ì˜ˆì‹œì™€ ë‚˜ìœ ì˜ˆì‹œ ##

    - **Question:** product_name = ['í• ë¦¬ìŠ¤ì»¤í”¼ì¡°ê°ì¼€ìµ'], expense = [10000]
    - **Context:** ... [í•­ëª©ë¶„ë¥˜ì§‘] ì¼€ì´í¬: 1085(ì¼€ì´í¬) ... [ì¡°ì‚¬ì‚¬ë¡€ì§‘] ì»¤í”¼ìˆ êµ¬ë§¤ ì¡°ê° ì¼€ìµ: 7560(ì£¼ì Â·ì»¤í”¼ìˆ) ...
    - **ë‚˜ìœ íŒë‹¨:** 'ì¼€ì´í¬'ë¼ëŠ” ì¼ë°˜ ë¶„ë¥˜ë¥¼ ë³´ê³  `1085`ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒ.
    - **ì¢‹ì€ íŒë‹¨:** [ì •ë³´ ìš°ì„ ìˆœìœ„ ê·œì¹™]ì— ë”°ë¼ 'ì¡°ì‚¬ì‚¬ë¡€ì§‘'ì˜ `7560`ì„ ì„ íƒí•˜ê³  "DEFINITE"ë¡œ ë¶„ë¥˜.

    ---
    ## ì¶œë ¥ í˜•ì‹ (ì•„ë˜ í˜•ì‹ ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ë‹µ) ##

    ### A. ëª…í™•í•œ ê²½ìš° (DEFINITE):
    ```json
    {{
      "classification_type": "DEFINITE",
      "result": {{
        "input_code": "ì¶”ë¡ í•œ ìˆ«ì ì…ë ¥ì½”ë“œ",
        "confidence": "ì‹ ë¢°ë„ (ì˜ˆ: 95%)",
        "reason": "ì ˆëŒ€ ê·œì¹™ê³¼ ì •ë³´ ìš°ì„ ìˆœìœ„ ê·œì¹™ì— ì…ê°í•˜ì—¬ ì´ ì½”ë“œë¥¼ ì„ íƒí•œ ëª…í™•í•œ ì´ìœ .",
        "evidence": "ê·¼ê±°ë¡œ ì‚¬ìš©í•œ ê°€ì¥ í•µì‹¬ì ì¸ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©(ì²­í¬) í•˜ë‚˜ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬"
      }}
    }}
    ```

    ### B. ëª¨í˜¸í•œ ê²½ìš° (AMBIGUOUS)
    ```json
    {{
      "classification_type": "AMBIGUOUS",
      "reason_for_ambiguity": "ì™œ ë‹¨ì¼ ì½”ë“œë¡œ í™•ì •í•  ìˆ˜ ì—†ëŠ”ì§€ì— ëŒ€í•œ í•µì‹¬ ì´ìœ  (ì˜ˆ: 'ë³´í—˜ì˜ ì¢…ë¥˜(í™”ì¬, ê±´ê°•, ìš´ì „, ìë™ì°¨ ë“±)ê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ ì—¬ëŸ¬ í›„ë³´ê°€ ê°€ëŠ¥í•¨')",
      "candidates": [
        {{
          "input_code": "í›„ë³´ ì…ë ¥ì½”ë“œ 1"("í›„ë³´ ì…ë ¥ì½”ë“œ 1ì˜ í•­ëª©ëª…"),
          "confidence": "í›„ë³´ 1ì˜ ì‹ ë¢°ë„ (ì˜ˆ: 50%)",
          "reason": "ì´ ì½”ë“œê°€ í›„ë³´ì¸ ì´ìœ "
        }},
        {{
          "input_code": "í›„ë³´ ì…ë ¥ì½”ë“œ 2"("í›„ë³´ ì…ë ¥ì½”ë“œ 2ì˜ í•­ëª©ëª…"),,
          "confidence": "í›„ë³´ 2ì˜ ì‹ ë¢°ë„ (ì˜ˆ: 30%)",
          "reason": "ì´ ì½”ë“œê°€ í›„ë³´ì¸ ì´ìœ "
        }}
      ],
      "evidence": "íŒë‹¨ì— ì‚¬ìš©ëœ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©(ì²­í¬) í•˜ë‚˜ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬"
    }}
    ```
    ---
    HUMAN:
    #Question: {question}
    #Context: {context}
    Answer:
""")

# ë‹¨ì¼ í’ˆëª© ì²˜ë¦¬ ì „ìš© ì²´ì¸
classification_chain_single = (
    {"question": itemgetter("question"), "context": itemgetter("context")}
    | prompt_template_single
    | _llm_model
    | StrOutputParser()
)

def fmt_won(x):
    try:
        return f"{int(x):,}ì›"
    except Exception:
        return "0ì›"
        
# ========================================
# Streamlit UI (ì‹¬í”Œí•˜ê²Œ)
# ========================================
# í—¤ë”
# ì „ì—­ ì–´ëŠ ê³³(ì˜ˆ: í˜ì´ì§€ ì„¤ì • ì•„ë˜)ì— CSS ì£¼ì…
st.markdown("""
<style>
.categome-center { display:flex; justify-content:center; align-items:center; }
.categome-caption { text-align:center; color:#666; margin-bottom:30px; }
</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown('<div class="categome-center">', unsafe_allow_html=True)
if os.path.exists("assets/CateGOMe_kor.png"):
    st.image("assets/CateGOMe_kor.png", width=420)
else:
    st.title("ğŸ¤– CateGOMe")
st.markdown('</div>', unsafe_allow_html=True)

st.markdown("""
<div class="categome-caption">
ê°€ê³„ë™í–¥ì¡°ì‚¬ í•­ëª©ì½”ë“œ ìë™ë¶„ë¥˜ AI ì„œë¹„ìŠ¤<br>
ê°€ê³„ë¶€ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ í’ˆëª©ì„ ë¶„ë¥˜í•´ë“œë¦½ë‹ˆë‹¤
</div>
""", unsafe_allow_html=True)

# ----------------------------------------------------------
# ì„¸ì…˜ ìŠ¤í† ë¦¬ì§€ ê¸°ë³¸ê°’
# ----------------------------------------------------------
st.session_state.setdefault("results", None)        # ì „ì²´ ê²°ê³¼ ìºì‹œ
st.session_state.setdefault("last_file_name", None) # ì—…ë¡œë“œ íŒŒì¼ ë³€ê²½ ê°ì§€

# === ì—…ë¡œë”(ìœ ì¼) ===
uploaded_file = st.file_uploader(
    "ê°€ê³„ë¶€ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
    type=['png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp', 'tiff'],
    help="ë“œë˜ê·¸ ì•¤ ë“œë¡­ ë˜ëŠ” í´ë¦­í•˜ì—¬ íŒŒì¼ ì„ íƒ",
    key="main_uploader_v2",
)

# íŒŒì¼ ë°”ë€Œë©´ ê²°ê³¼ ì´ˆê¸°í™”
if uploaded_file is not None and st.session_state["last_file_name"] != uploaded_file.name:
    st.session_state["results"] = None
    st.session_state["last_file_name"] = uploaded_file.name

# ----------------------------------------------------------
# ë²„íŠ¼ (ì¤‘ì•™)
# ----------------------------------------------------------
if uploaded_file is not None:
    c1, c2, c3 = st.columns([2, 1, 2])
    with c2:
        run = st.button("ğŸš€ ë¶„ë¥˜ ì‹œì‘", type="primary", use_container_width=True, key="run_btn_v2")

    # ======================================================
    # 1) ë¬´ê±°ìš´ íŒŒì´í”„ë¼ì¸: ë²„íŠ¼ ëˆŒë €ì„ ë•Œë§Œ ì‹¤í–‰
    #    ì‹¤í–‰ ê²°ê³¼ëŠ” session_state["results"]ì— ì €ì¥
    # ======================================================
    if run:
        progress = st.progress(0, "ì´ë¯¸ì§€ ë¶„ì„ ì¤€ë¹„ ì¤‘...")

        # --- ì—¬ê¸°ëŠ” ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ê·¸ëŒ€ë¡œ (OCR â†’ ê²€ìƒ‰ â†’ LLM) ---
        #     ë‹¨, ë§ˆì§€ë§‰ì— df_definite / ambiguous_results / failed_resultsë§Œ ì €ì¥í•´ì£¼ë©´ ë©ë‹ˆë‹¤.
        img = Image.open(uploaded_file).convert("RGB")
        progress.progress(20, "ğŸ“¸ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")

        gemini_model = genai.GenerativeModel("gemini-1.5-flash")

        prompt = """
ê°€ê³„ë¶€ ì‚¬ì§„ì—ì„œ í‘œë¥¼ ì¸ì‹í•´ì„œ ê° í–‰ì˜
1) í’ˆëª©ëª…(= 'ìˆ˜ì…ì¢…ë¥˜ ë° ì§€ì¶œì˜ í’ˆëª…ê³¼ ìš©ë„' ì—´),
2) ìˆ˜ì… ê¸ˆì•¡,
3) ì§€ì¶œ ê¸ˆì•¡
ì„ ì¶”ì¶œí•˜ë¼.

ê·œì¹™:
- ê¸ˆì•¡ì˜ ì‰¼í‘œ(,)ëŠ” ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ.
- ê°’ì´ ë¹„ì–´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ.
- ì œëª©í–‰Â·ì²´í¬ë°•ìŠ¤Â·ë¹ˆì¤„ì€ ì œì™¸.
- ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥.

JSON ìŠ¤í‚¤ë§ˆ:
{
  "items": [
    {"name": "í’ˆëª©ëª…", "income": 0, "expense": 0},
    ...
  ]
}
"""
        img_bytes = uploaded_file.getvalue()
        resp = gemini_model.generate_content(
            [{"text": prompt}, {"inline_data": {"mime_type": uploaded_file.type, "data": img_bytes}}],
            generation_config={"response_mime_type": "application/json"}
        )
        raw = resp.text
        data = json.loads(raw)

        # í›„ì²˜ë¦¬
        items = []
        for it in data.get("items", []):
            name = str(it.get("name", "")).strip()
            def to_int(x):
                s = str(x).replace(",", "").strip()
                return int(re.sub(r"[^\d]", "", s)) if re.search(r"\d", s) else 0
            income = to_int(it.get("income", 0))
            expense = to_int(it.get("expense", 0))
            if name:
                items.append({"name": name, "income": income, "expense": expense})

        product_name_list = [it["name"] for it in items]
        income_list        = [it["income"] for it in items]
        expense_list       = [it["expense"] for it in items]

        progress.progress(30, f"âœ… {len(items)}ê°œ í’ˆëª© ë°œê²¬")

        # ì½”ë“œâ†’í•­ëª©ëª… ë§µ
        _df['ì…ë ¥ì½”ë“œ_str'] = _df['ì…ë ¥ì½”ë“œ'].astype(str).str.replace(r'\.0$', '', regex=True)
        code_to_name_map = pd.Series(_df.í•­ëª©ëª….values, index=_df.ì…ë ¥ì½”ë“œ_str).to_dict()

        # ë²¡í„°ìŠ¤í† ì–´ ë¬¸ì„œ ë©”ëª¨ë¦¬ ë¡œë“œ
        all_docs_from_vs = {name: list(vs.docstore._dict.values()) for name, vs in _vectorstores.items()}

        # ê²°ê³¼ ì»¨í…Œì´ë„ˆ
        definite_results, ambiguous_results, failed_results = [], [], []

        total = max(len(product_name_list), 1)
        for i, pname_orig in enumerate(product_name_list):
            progress.progress(30 + int(60 * (i + 1) / total), f"ğŸ” ë¶„ë¥˜ ì¤‘... ({i+1}/{total}) - {pname_orig}")

            q_single = f"product_name = ['{pname_orig}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
            search_output = search_classification_codes(q_single, all_docs_from_vs, sim_topk_per_term=3, num_related_terms=3)
            pname = (search_output.get("extracted_terms_info") or [{"term": pname_orig}])[0]["term"]

            if "error" in search_output or not search_output["context_docs"]:
                failed_results.append({"í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"})
                continue

            context = "\n\n---\n\n".join([d.page_content for d in search_output["context_docs"]])
            context = context.replace("ì¶œì²˜: cases", "ì¶œì²˜: ì¡°ì‚¬ì‚¬ë¡€ì§‘").replace("ì¶œì²˜: classification", "ì¶œì²˜: í•­ëª©ë¶„ë¥˜ì§‘")

            final_question = f"product_name = ['{pname}'], income = [{income_list[i]}], expense = [{expense_list[i]}]"
            input_data = {"question": final_question, "context": context}

            try:
                out_str = classification_chain_single.invoke(input_data)
                m = re.search(r'\{.*\}', out_str, re.DOTALL)
                llm = json.loads(m.group(0)) if m else {}
                ctype = llm.get("classification_type")

                if ctype == "DEFINITE":
                    r = llm.get("result", {})
                    code = str(r.get("input_code", "")).strip()
                    item_name = code_to_name_map.get(code, "í•­ëª©ëª… ì—†ìŒ")
                    definite_results.append({
                        "í’ˆëª©ëª…": pname, "ì…ë ¥ì½”ë“œ": code, "í•­ëª©ëª…": item_name,
                        "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i],
                        "ì‹ ë¢°ë„": r.get("confidence","N/A"),
                        "ì¶”ë¡  ì´ìœ ": r.get("reason","N/A"), "ê·¼ê±°ì •ë³´": r.get("evidence","N/A")
                    })
                elif ctype == "AMBIGUOUS":
                    cands = llm.get("candidates", [])
                    for c in cands:
                        c["í•­ëª©ëª…"] = code_to_name_map.get(str(c.get("input_code","")).strip(), "í•­ëª©ëª… ì—†ìŒ")
                    ambiguous_results.append({
                        "í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i],
                        "ëª¨í˜¸ì„± ì´ìœ ": llm.get("reason_for_ambiguity","N/A"),
                        "í›„ë³´": cands, "ê·¼ê±°ì •ë³´": llm.get("evidence","N/A")
                    })
                else:
                    failed_results.append({"í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…: {ctype}"})
            except Exception as e:
                failed_results.append({"í’ˆëª©ëª…": pname, "ìˆ˜ì…": income_list[i], "ì§€ì¶œ": expense_list[i], "ì‹¤íŒ¨ ì´ìœ ": str(e)})

        # ----- DataFrame ìƒì„± ë° ìˆ«ìí˜•ìœ¼ë¡œ ê°•ì œ(âš ï¸ ì œê±° í•µì‹¬) -----
        df_definite = pd.DataFrame(definite_results)
        if not df_definite.empty:
            for col in ["ìˆ˜ì…", "ì§€ì¶œ"]:
                df_definite[col] = pd.to_numeric(df_definite[col], errors="coerce").fillna(0).astype(int)

        # ìºì‹œì— ì €ì¥ (ë‹¤ìŒ rerunì—ì„œ ì¬ì‚¬ìš©)
        st.session_state["results"] = {
            "df_definite": df_definite,
            "ambiguous_results": ambiguous_results,
            "failed_results": failed_results,
        }
        
        progress.progress(100, "âœ… ë¶„ë¥˜ ì™„ë£Œ!")

# ======================================================
# 2) ë Œë”ë§: resultsê°€ ìˆìœ¼ë©´ ì¬ê³„ì‚° ì—†ì´ ê·¸ëŒ€ë¡œ í‘œì‹œ
#    (ì²´í¬ë°•ìŠ¤ ëˆŒëŸ¬ë„ â€˜ë‹¤ì‹œ ë¶„ë¥˜â€™ ì•ˆ ëŒì•„ê°)
# ======================================================
results = st.session_state.get("results")
if results is not None:
    df_definite        = results["df_definite"]
    ambiguous_results  = results["ambiguous_results"]
    failed_results     = results["failed_results"]
    
    st.markdown("---")
    st.markdown("## ğŸ“Š ë¶„ë¥˜ ê²°ê³¼")

    # --- (1) ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª© ---
    if not df_definite.empty:
        st.markdown("### âœ… ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©")
        view_def = df_definite.copy()
        view_def["ìˆ˜ì…(ì›)"] = view_def["ìˆ˜ì…"].apply(fmt_won)
        view_def["ì§€ì¶œ(ì›)"] = view_def["ì§€ì¶œ"].apply(fmt_won)
        view_def = view_def[["í’ˆëª©ëª…", "ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ìˆ˜ì…(ì›)", "ì§€ì¶œ(ì›)"]]
        sty = (
            view_def
            .style
            .set_properties(subset=["ìˆ˜ì…(ì›)", "ì§€ì¶œ(ì›)"], **{"text-align": "right"})
        )
        # st.tableì€ Stylerë¥¼ ë°˜ì˜í•´ ì •ë ¬ì´ ë¨¹ìŒ
        st.table(sty)
    
    # --- (2) ì…ë ¥ì½”ë“œë³„ ìš”ì•½ ë³´ê¸° (ì¬ê³„ì‚° ì—†ì´ ìºì‹œë¡œë¶€í„°) ---
    if st.checkbox("ì…ë ¥ì½”ë“œë³„ ìš”ì•½ ë³´ê¸°", key="show_summary"):
        if not df_definite.empty:
            numeric_codes_mask = pd.to_numeric(df_definite['ì…ë ¥ì½”ë“œ'], errors='coerce').notna()
            df_summary = df_definite[numeric_codes_mask].copy()
            
            if not df_summary.empty:
                df_summary['ì…ë ¥ì½”ë“œ'] = df_summary['ì…ë ¥ì½”ë“œ'].astype(float).astype(int)
                df_summary_agg = df_summary.groupby('ì…ë ¥ì½”ë“œ').agg(
                    í•­ëª©ëª…=('í•­ëª©ëª…', 'first'),
                    ìˆ˜ì…í•©ê³„=('ìˆ˜ì…', 'sum'),
                    ì§€ì¶œí•©ê³„=('ì§€ì¶œ', 'sum'),
                    í•´ë‹¹í’ˆëª©ëª…=('í’ˆëª©ëª…', lambda x: ', '.join(x))
                ).reset_index()
                
                view_sum = df_summary_agg.copy()
                view_sum["ìˆ˜ì…í•©ê³„(ì›)"] = view_sum["ìˆ˜ì…í•©ê³„"].apply(fmt_won)
                view_sum["ì§€ì¶œí•©ê³„(ì›)"] = view_sum["ì§€ì¶œí•©ê³„"].apply(fmt_won)
                view_sum = view_sum[['ì…ë ¥ì½”ë“œ', 'í•­ëª©ëª…', 'ìˆ˜ì…í•©ê³„(ì›)', 'ì§€ì¶œí•©ê³„(ì›)', 'í•´ë‹¹í’ˆëª©ëª…']]
                
                sty2 = (
                    view_sum
                    .style
                    .set_properties(subset=["ìˆ˜ì…í•©ê³„(ì›)", "ì§€ì¶œí•©ê³„(ì›)"], **{"text-align": "right"})
                )
                # st.tableì€ Stylerë¥¼ ë°˜ì˜í•´ ì •ë ¬ì´ ë¨¹ìŒ
                st.table(sty2)
            else:
                st.warning("ìˆ«ì ì½”ë“œê°€ ìˆëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ëª…í™•í•˜ê²Œ ë¶„ë¥˜ëœ í’ˆëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # --- (3) ëª…í™•í•œ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±° ---
    if not df_definite.empty:
        with st.expander("ğŸ” ëª…í™•í•œ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±°", expanded=False):
            for row in df_definite.to_dict(orient="records"):
                st.markdown(
                    f"**í’ˆëª©ëª…: {row['í’ˆëª©ëª…']} (ì„ íƒëœ ì½”ë“œ: {row['ì…ë ¥ì½”ë“œ']}, "
                    f"í•­ëª©ëª…: {row['í•­ëª©ëª…']}, ì‹ ë¢°ë„: {row['ì‹ ë¢°ë„']})**"
                )
                if row.get("ì¶”ë¡  ì´ìœ "):
                    st.write(f"**- ì¶”ë¡  ì´ìœ :** {row['ì¶”ë¡  ì´ìœ ']}")
                if row.get("ê·¼ê±°ì •ë³´"):
                    st.write("**- í•µì‹¬ ê·¼ê±°:**")
                    st.code(row["ê·¼ê±°ì •ë³´"])
                st.markdown("---")
    
    # --- (4) ì‚¬ìš©ì ê²€í† ê°€ í•„ìš”í•œ í’ˆëª© (ì»¬ëŸ¼ëª… í•œê¸€í™”) ---
    if ambiguous_results:
        st.markdown("### âš ï¸ ì‚¬ìš©ì ê²€í† ê°€ í•„ìš”í•œ í’ˆëª©")
        st.info("ì•„ë˜ í’ˆëª©ë“¤ì€ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹¨ì¼ ì½”ë“œë¥¼ í™•ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        for result in ambiguous_results:
            with st.expander(f"ğŸ“Œ {result['í’ˆëª©ëª…']} (ìˆ˜ì…: {fmt_won(result['ìˆ˜ì…'])}, ì§€ì¶œ: {fmt_won(result['ì§€ì¶œ'])})"):
                st.write(f"**ê²€í†  í•„ìš” ì´ìœ :** {result['ëª¨í˜¸ì„± ì´ìœ ']}")
                candidates_df = pd.DataFrame(result['í›„ë³´']).rename(columns={
                    "input_code": "ì…ë ¥ì½”ë“œ",
                    "confidence": "ì‹ ë¢°ë„",
                    "reason": "ê·¼ê±°ì •ë³´",
                })
                # í‘œì‹œ ì»¬ëŸ¼ ìˆœì„œ ê³ ì •
                view_cols = [c for c in ["ì…ë ¥ì½”ë“œ", "í•­ëª©ëª…", "ì‹ ë¢°ë„", "ê·¼ê±°ì •ë³´"] if c in candidates_df.columns]
                h3 = min(44 * (len(candidates_df) + 1), 400)
                st.dataframe(candidates_df[view_cols], use_container_width=True, height=h3, hide_index=True)
    
    # --- (5) ì‹¤íŒ¨ í•­ëª© ---
    if failed_results:
        with st.expander("âŒ ì²˜ë¦¬ ì‹¤íŒ¨ í•­ëª©"):
            df_failed = pd.DataFrame(failed_results)
            h4 = min(44 * (len(df_failed) + 1), 400)
            st.dataframe(df_failed, use_container_width=True, height=h4, hide_index=True)
